{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj5iJRgIpkCP",
        "outputId": "43c85513-27cd-4043-b398-5939712b28a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.7/409.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVXbDYwEp-LQ",
        "outputId": "961d7439-7afc-4af4-e038-aa56814b9a3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load pdf file from url"
      ],
      "metadata": {
        "id": "J4hOIxSjpCOR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RxVa7hA5o-Jp"
      },
      "outputs": [],
      "source": [
        "url = 'https://arxiv.org/pdf/2312.16862.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load per page"
      ],
      "metadata": {
        "id": "caqRvJcNpgTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(url)\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "H_shXWw5pels"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj52ufP7p0Vl",
        "outputId": "46630b28-5c05-4b77-8d9d-3641c5b91b75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgWGerRBp1ws",
        "outputId": "54f7958d-dcac-4d0d-8577-6f583ef4c974"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 0}, page_content='TinyGPT-V: Efficient Multimodal Large Language Model\\nvia Small Backbones\\nZhengqing Yuan1 Zhaoxu Li 2 Weiran Huang3 Yanfang Ye1 Lichao Sun 4\\nAbstract\\nIn recent years, multimodal large language mod-\\nels (MLLMs) such as GPT-4V have demonstrated\\nremarkable advancements, excelling in a variety\\nof vision-language tasks. Despite their prowess,\\nthe closed-source nature and computational de-\\nmands of such models limit their accessibility and\\napplicability. This study introduces TinyGPT-V ,\\na novel open-source MLLM, designed for effi-\\ncient training and inference across various vision-\\nlanguage tasks, including image captioning (IC)\\nand visual question answering (VQA). Leverag-\\ning a compact yet powerful architecture, TinyGPT-\\nV integrates the Phi-2 language model with pre-\\ntrained vision encoders, utilizing a unique map-\\nping module for visual and linguistic information\\nfusion. With a training regimen optimized for\\nsmall backbones and employing a diverse dataset\\namalgam, TinyGPT-V requires significantly lower\\ncomputational resources—24GB for training and\\nas little as 8GB for inference—without compro-\\nmising on performance. Our experiments demon-\\nstrate that TinyGPT-V , with its language model\\n2.8 billion parameters, achieves comparable re-\\nsults in VQA and image inference tasks to its\\nlarger counterparts while being uniquely suited\\nfor deployment on resource-constrained devices\\nthrough innovative quantization techniques. This\\nwork not only paves the way for more accessible\\nand efficient MLLMs but also underscores the po-\\ntential of smaller, optimized models in bridging\\nthe gap between high performance and computa-\\ntional efficiency in real-world applications. Addi-\\ntionally, this paper introduces a new approach to\\nmultimodal large language models using smaller\\nbackbones. Our code and training weights are\\navailable in the supplementary material.\\n1Universiy of Notre Dame 2Nanyang Technological University\\n3Shanghai Jiao Tong University 4Lehigh University. Correspon-\\ndence to: Lichao Sun <lis221@lehigh.edu>.\\nAccepted to the Workshop on Advancing Neural Network Training\\nat International Conference on Machine Learning (W ANT@ICML\\n2024).\\n5.07.510.012.515.0\\nParameter (Billions)\\n38\\n40\\n42\\n44\\n46Performance Flamingo\\nBLIP-2\\nLLaVA\\nInstructBLIP\\nMiniGPT-4\\nTinyGPT-V\\nFigure 1: Comparison of TinyGPT-V with other current\\nMLLMs models shows TinyGPT-V achieves cost-effective,\\nefficient, and high-performing with fewer parameters.\\n1. Introduction\\nIn recent years, the field of artificial intelligence has seen\\nsignificant advancements through the development of multi-\\nmodal large language models (MLLMs), such as GPT-4V ,\\nwhich have shown exceptional performance across a range\\nof vision-language tasks (Yang et al., 2023). Despite GPT-\\n4V’s impressive capabilities, its closed-source nature limits\\nits widespread application and adaptability. In contrast, the\\nopen-source landscape for MLLMs is rapidly evolving, pre-\\nsenting models like LLaV A and MiniGPT-4 that excel in\\nimage captioning (IC), visual question answering (VQA)\\noften comparable GPT-4V in these areas (Dai et al., 2023;\\nLiu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-\\nv2 (Chen et al., 2023) has demonstrated superior perfor-\\nmance in various visual grounding and question-answering\\ntasks. However, its training code remains proprietary, which\\nposes challenges for community-driven advancements and\\nadaptability.\\nAlthough the impressive vision-language capabilities\\ndemonstrated by some open-source MLLMs, they fre-\\nquently necessitate significant computational resources for\\ntraining and inference. For example, training LLaV A-v1.5-\\n13B (Liu et al., 2023a) required 8 × A100 GPUs, each\\nequipped with 80GB of memory, cumulating in 25.5 hours\\nof continuous training. As shown in Figure 2 (a), the un-\\n1\\narXiv:2312.16862v3  [cs.CV]  21 Jun 2024'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 1}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nProjection (110M)\\n0.8%\\nEVA-VIT (0.98B)\\n7.0%\\nVicuna (13B)\\n92.3%\\nProjection (119M)\\n3.1%\\nEVA-VIT (0.98B)\\n25.1%\\nPhi-2 (2.8B)\\n71.8%\\n(a) MiniGPT-4 (a) TinyGPT-V\\nFigure 2: (a) is the occupancy ratio of each component in MiniGPT-4, and (b) is the occupancy ratio of each component in\\nTinyGPT-V . We have considerably narrowed down the occupancy ratio of the Language model in MLLMs.\\nderlying performance of large language models, which are\\nintegral to MLLMs, is pivotal. Models such as LLaV A-\\nv1.5-13B and MiniGPT-v2 depend on high-capacity back-\\nbones which are Vicuna-13b-v1.5 (Zheng et al., 2023) and\\nLLaMA2-7B-Chat (Touvron et al., 2023), respectively, ne-\\ncessitating a substantial number of parameters to effectively\\ntackle complex tasks including IC and VQA.\\nWe introduce TinyGPT-V , a novel model designed for effi-\\ncient training and inference, requiring only 24GB of GPU\\nmemory for training and as little as 8GB of GPU or CPU\\nmemory for inference. This model makes use of the ad-\\nvanced large language model Phi-2 (Javaheripi et al., 2023)\\nand incorporates pre-trained vision modules from (Li et al.,\\n2023a) and CLIP (Radford et al., 2021) as its vision encoder,\\ncoupled with a mapping module to facilitate the integration\\nof visual information. During training, TinyGPT-V adopts\\na novel training methodology focused on small pre-trained\\nbackbones, unlike any other MLLMs, utilizing the unique\\nmapping module between the visual encoder and the lan-\\nguage model as well as novelty normalization methods,\\nwhile keeping all other components frozen. For its train-\\ning dataset, TinyGPT-V employs the multi-tasks datasets,\\nincluding LAION (Schuhmann et al., 2021), Conceptual\\nCaptions (Changpinyo et al., 2021; Sharma et al., 2018),\\nSBU (Ordonez et al., 2011), and others (Lin et al., 2015;\\nSchwenk et al., 2022; Hudson & Manning, 2019; Kiela et al.,\\n2020; Lu et al., 2021; Gurari et al., 2018; Mao et al., 2016;\\nKazemzadeh et al., 2014; Yu et al., 2016).\\nIn our study, we found that TinyGPT-V exhibits similar traits\\nwith GPT-4, especially when doing some VQA and image\\ninference. With only 2.8 billion parameters of its language\\nmodel, TinyGPT-V employs a unique quantization process,\\nlike using 8-bit quantization, making it well-suited for local\\ndeployment and inference on 8GB mobile devices. This\\nmodel represents a significant advancement in achieving a\\nbalance between exceptional performance and efficiency in\\nMLLMs, as shown in Figure 1. Our work not only aims\\nto enable the community to develop more cost-effective,\\nefficient, and high-performing MLLMs for widespread real-\\nworld applications but also introduces a training framework\\noptimized for small pre-trained backbones.\\n2. Related Work\\nAdvanced language model. The evolution of language\\nmodels has been marked by significant milestones, start-\\ning with early successes like GPT2 (Radford et al., 2019)\\nand BERT (Devlin et al., 2018) in natural language process-\\ning (NLP). These foundational models set the stage for the\\nsubsequent development of vastly larger language models,\\nencompassing hundreds of billions of parameters. This dra-\\nmatic increase in scale has led to the emergence of advanced\\ncapabilities as seen in models like GPT-3 (Brown et al.,\\n2020), Chinchilla (Hoffmann et al., 2022), OPT (Zhang\\net al., 2022), and BLOOM (Workshop et al., 2022). These\\nlarge language models (LLMs) have been instrumental\\nin further advancements in the field. For instance, Chat-\\nGPT (OpenAI, 2022) and InstructGPT (Ouyang et al., 2022)\\nleverage these powerful models to answer diverse ques-\\ntions and perform complex tasks such as coding. The in-\\ntroduction of open-source LLMs like LLaMA (Touvron\\net al., 2023) has further propelled research in this area, in-\\nspiring subsequent developments like Alpaca (Taori et al.,\\n2023), Vicuna (Chiang et al., 2023). These models fine-tune\\nthe LLaMA model with additional high-quality instruction\\ndatasets, showcasing the versatility and adaptability of LLM\\nframeworks.Among the most notable recent advancements\\nare Phi (Li et al., 2023b) and its successor, Phi-2 (Javaheripi\\net al., 2023). These models have demonstrated exceptional\\nperformance, rivaling or even surpassing models up to 25\\ntimes larger in scale. This indicates a significant shift in\\n2'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 2}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nFigure 3: Compared to other general-purpose MLLMs, our TinyGPT-V achieves the same performance as 13B or 7B models\\nin a variety of visual language tasks.\\nthe landscape of language modeling, emphasizing efficiency\\nand effectiveness without necessarily relying on sheer size.\\nMultimodal language model. In recent years, the trend of\\naligning visual input to large language models for vision-\\nlanguage tasks has gained significant attention (Chen et al.,\\n2022; Tsimpoukelli et al., 2021; Alayrac et al., 2022; Li\\net al., 2023a; Liu et al., 2023b;a; Zhu et al., 2023; Chen\\net al., 2023). Seminal works like VisualGPT (Chen et al.,\\n2022) and Frozen (Tsimpoukelli et al., 2021), which uti-\\nlized pre-trained language models for image captioning and\\nvisual question answering. This approach was further ad-\\nvanced by models such as Flamingo (Alayrac et al., 2022),\\nwhich incorporated gated cross-attention mechanisms to\\nalign pre-trained vision encoders and language models, train-\\ning on vast image-text pairs. BLIP-2 (Li et al., 2023a)\\nintroduced an efficient Q-Former for aligning visual and\\nlanguage modalities. These groundbreaking studies have\\npaved the way for further innovations in the field, leading to\\nthe development of models like LLaV A (Liu et al., 2023b)\\nand MiniGPT4 (Zhu et al., 2023), and their subsequent iter-\\nations, LLaV A-v1.5 (Liu et al., 2023a), MiniGPT-v2 (Chen\\net al., 2023), ArtGPT-4 (Yuan et al., 2023), instruction GPT-\\n4 (Wei et al., 2023) and Instruction Mining (Cao et al.,\\n2023). These models have demonstrated advanced multi-\\nmodal capabilities through instruction tuning, showcasing\\nremarkable generalization abilities.\\n3. Method\\nWe briefly introduce our vision-language model, TinyGPT-\\nV , followed by an analysis of its structure, culminating in a\\ndetailed description of the training process for each stage.\\n3.1. Model Architecture\\nIn this subsection, we present the architecture of TinyGPT-V ,\\nwhich consists of a visual encoder, projection layers, and a\\nlarge language model, as shown in Figure 4.\\nVisual encoder backbone. In the TinyGPT-V , it utilizes\\nEV A (Fang et al., 2022) of the ViT serves as the visual\\nfoundation model, which remains inactive during the entire\\ntraining process. Our model operates at an image resolution\\nof 224x224 for Stages 1, 2, and 3, and at 448x448 for Stage\\n4. The positional encoding is enhanced to accommodate the\\nincreased image resolution which is known as the Relative\\nPosition Bias (Dufter et al., 2021). It enhances the model’s\\nunderstanding of the spatial relationships between elements\\nin an image.\\nProjection layers. The Projection layers embed visual\\nfeatures extracted by the visual encoder into the language\\nmodel, enhancing the model’s ability to process image-\\nbased information. We adopt the Q-Former layers from\\nthe BLIP-2 architecture (Li et al., 2023a) as the initial pro-\\n3'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 3}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nPhi-2\\nLoRA+Normalization\\nFreeze Train\\nQ-Former\\nViT\\nLinear 1 (MiniGPT-4)\\nLinear 2\\nThis is an image of an alpaca. \\nAlpacas are domesticated \\nspecies of South American \\ncamelids, known for their \\nsoft fluffy coats which are \\nused for making wool.\\n[vqa]What’s this?\\nFigure 4: Architecture of TinyGPT-V . The model takes a\\nvisual backbone, which remains frozen during all training\\nphases. We concatenate Q-Former layer visual output tokens\\nfrom ViT backbone and project them into Phi-2 language\\nmodel space via two linear projection layers.\\njection layer, aiming to leverage the full potential of the\\npre-trained BLIP system within visual language models.\\nThis strategy significantly reduces the number of parameters\\nneeding training. The second and third layers are linear\\nprojection layers, designed to bridge the dimensionality gap\\nbetween the Q-Former output and the language model’s\\nembedding layer, thereby aligning visual tokens more effec-\\ntively with the language model’s hidden space. As shown in\\nFigure 6, to expedite TinyGPT-V’s training, we initially use\\na pre-trained Linear Projection from MiniGPT-4 (Vicuna\\n7B) as the second layer. We then introduce an additional lin-\\near projection layer, initialized with a Gaussian distribution,\\nas the third layer to seamlessly integrate into the hidden\\nspace of the Phi-2 model.\\nLarge lanuguage model backbone. Our TinyGPT-V large\\nlanguage model is built upon the Phi-2 model (Javaheripi\\net al., 2023) as its backbone. Phi-2, a 2.7 billion-parameter\\nlanguage model, exhibits exceptional reasoning and lan-\\nguage comprehension abilities, achieving state-of-the-art\\nperformance among language models with fewer than 13\\nbillion parameters. In complex benchmarks, Phi-2 either\\nmatches or exceeds the performance of models up to 25\\ntimes its size. We primarily use Phi-2’s linguistic abilities\\nto do various vision-language tasks. Specifically, for vision\\nreasoning tasks that involve spatial location identification,\\nwe instruct the linguistic model to generate textual descrip-\\ntions of what will happen in the next scenario, representing\\ntheir objects’ coordinates, as shown in Table 8.\\nNormalization and LoRA for TinyGPT-V .In Section 4.4,\\nwe conclude that training smaller-scale large language mod-\\nels for transfer learning, especially across different modali-\\nties (e.g., from text to image), poses significant challenges.\\nOur studies indicate that these smaller models are prone to\\nencountering NaN or INF values during multimodal data\\ncomputations. This issue often leads to a computational\\nloss value of NaN, thereby causing failure in the initial\\nbatch forward propagation. Moreover, the limited number\\nof trainable parameters in these models may lead to gradient\\nvanishing during training. To mitigate these problems, as\\ndepicted in Figure 5 (c), we incorporate the post-norm and\\ninput norm mechanisms from LLaMA-2, applying RMS\\nNorm after each Multi-Head Attention Layer (MHA) to\\nnormalize data for downstream processing. In addition, we\\nhave to update all layer norms in the Phi-2 model to improve\\ntraining stability, as detailed in the subsequent equation.\\nLayerNorminput(xhidden) =γ xhidden − µ√\\nσ2 + ϵ\\n+ β (1)\\nWhere, xhidden is the input of this layer, µ and σ2 are the\\nmean and variance of the inputs to the layer, respectively,ϵ\\nis a small number to prevent division by zero, γ and β are\\ntrainable parameters.\\nRMSNorm(xpost) = xpostq\\n1\\nN\\nPN\\ni=1 x2\\ni + ϵ\\n(2)\\nwhere xpost is the input after MHA, N is the dimension of\\nxpost.\\nFurthermore, (Henry et al., 2020) have underscored the vital\\nrole of Query-Key Normalization in low-resource learning\\nscenarios. Hence, as show in Figure 5 (d), we have incor-\\nporated Query-Key Normalization into the Phi-2 model, as\\ndetailed in the following equation.\\nAttention(Q, K, V) =softmax\\n\\x12LayerNorm(Q)LayerNorm(K)T\\n√dk\\n\\x13\\nV\\n(3)\\nwhere dk denotes the dimension of Q or K.\\nThe structure of the LoRA mechanism (Hu et al., 2021)\\nis show in Figure 5 (a), which is an efficient fine-tuning\\nmethod in parallel to the frozen pre-training weights as\\nshown in Figure 5 (c), which does not increase the inference\\ntime consuming for large language models and is easier to\\noptimize.\\n3.2. Training Stages\\nIn this subsection, the four-stage training process of\\nTinyGPT-V will be described.\\nWarm-up training for the first training stage. During\\nthe initial pretraining stage, TinyGPT-V is taught vision-\\nlanguage understanding using large datasets of aligned\\n4'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 4}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nLinear\\nDown\\nr\\nLinear\\nUpPretrained\\nWeights\\n\\ud835\\nInput\\nOutput\\nLoRA\\n(a) LoRA\\nMLP\\nNormalization\\nMHA LoRA\\n(b) LoRA Module \\nfor LLMs Block\\nMHA\\nQuery-Key \\nNormalization\\nLayer Norm\\nRMS Norm\\nMLP\\nLoRA\\nAfter \\nStage1\\n(c) LLMs Block\\n for TinyGPT-V\\nAttention\\nQ K V\\nLayer \\nNorm\\nLayer \\nNorm\\n(d) Query-Key \\nNormalization for MHA\\nFreeze Train Data Pathway Conditional Pathway\\nFigure 5: (a) represents the structure of LoRA, (b) represents how LoRA can efficiently fine-tune large language models\\n(LLMs) in natural language processing, (c) represents the structure of LLMs for TinyGPT-V , and (d) represents the structure\\nof QK Normalization.\\nPhi-2\\nImage-text Pair \\nInstruction Learning\\nFreezeTrain\\nStage 1\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\nPhi-2\\nStage 2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nPhi-2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nStage 3\\nPhi-2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nMulti-Tasks Learning\\nStage 4\\nFigure 6: The training process of TinyGPT-V , the first stage\\nis warm-up training, the second stage is pre-training, the\\nthird stage is instruction fine-tuning, and the fourth stage is\\nmulti-task learning.\\nimage-text pairs. The model identifies the output from the\\nprojection layers as a soft prompt directing it to create rel-\\nevant texts and to allow large language models to accept\\ninputs from the image modality. The pretraining process\\nuses a dataset combination of Conceptual Caption, SBU,\\nand LAION, involving 20000 training steps covering about\\n5 million image-text pairs.\\nPre-training for the second training stage. Following the\\ninitial training stage, the large language model becomes\\nequipped to process image modality inputs. To guarantee\\nmore consistent performance as the model transitions into\\nthe subsequent training stage, we re-employ the dataset from\\nthe first stage, specifically for training the LoRA module.\\nInstruction tuning for the third training stage. We fine-\\ntuned this TinyGPT-V model using a selection of image-text\\npairings from MiniGPT4 or LLaV A, which included in-\\nstructions like “###Human: <Img><ImageHere></Img>\\nTake a look at this image and describe what you no-\\ntice.###Assistant:.”. We used a uniform template inclusive\\nof a randomly chosen prompt that improved the model’s\\ncapacity for generating responses that were consistent and\\nsounded more natural.\\nMulti-task learning in the fourth training stage. The\\nfourth training stage of TinyGPT-V focuses on enhancing\\nits conversation ability as a chatbot by tuning the model with\\nmore multi-modal instruction datasets as shown in Table 1,\\nincluding LLaV A, Flickr30k, a mixing multi-task dataset,\\nand Unnatural Instruction using multi-tasks template as de-\\ntailed in appendix A. The LLaV A dataset is utilized for\\nmulti-modal instruction tuning with detailed descriptions\\nand complex reasoning examples. The Flickr30k dataset\\nis used to improve grounded image caption generation and\\nobject parsing and grounding capabilities. Additionally, a\\nmixing multi-task dataset is created to improve the model’s\\nhandling of multiple tasks during multi-round conversations.\\nFinally, to recover the language generation ability, the Un-\\nnatural Instruction dataset is added to the third-stage training\\nof TinyGPT-V .\\n5'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 5}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nData types Dataset Stage 1 Stage 2 Stage 3 Stage 4\\nImage-text pair LAION, CC3M, SBU ✓ ✓ ✗ ✗\\nInstruction tuning MiniGPT-4 Stage2 for CC & SBU ✗ ✗ ✓ ✗\\nCaption Text Captions, COCO Captions ✗ ✗ ✗ ✓\\nREC RefCOCO, RefCOCO+, RefCOCOg, Visual Genome ✗ ✗ ✗ ✓\\nVQA GQA, VQAv2, OK-VQA, AOK-VQA, OCR-VQA ✗ ✗ ✗ ✓\\nMultimodal instruction LLaV A dataset, Flickr30k, Multi-task conversation ✗ ✗ ✗ ✓\\nLangauge dataset Unnatural Instructions ✗ ✗ ✗ ✓\\nTable 1: The full list of datasets used by TinyGPT-MoE during training.\\nFigure 7: Changes in loss during the training stage of TinyGPT-V .\\n4. Experiments\\nIn this section, we describe the training and evaluation meth-\\nods in detail.\\n4.1. Training\\nExperimental setting. The experimental environment for\\nthis study was established with a single NVIDIA RTX 3090\\nGPU, equipped with a substantial 24GB of VRAM. The cen-\\ntral processing was handled by an AMD EPYC 7552 48-core\\nProcessor, offering 15 virtual CPUs. Memory allocation was\\nset at 80GB, ensuring sufficient capacity for handling large\\ndatasets. The software environment was standardized on\\nPyTorch version 2.0.0, with CUDA 11.8 support, facilitating\\noptimized tensor operations on the GPU.\\nTraining process. In our experimental process, we meticu-\\nlously orchestrated the training of our model through four\\ndistinct stages, each characterized by specific learning rate\\nstrategies and loss profiles, as shown in Figure 7.\\nStage 1: Spanning 17 epochs, with each epoch consisting\\nof 1000 iterations, we employed a dynamic learning rate\\napproach. The learning rate commenced at 1e-5 at the begin-\\nning of each epoch and gradually ascended to 1e-4 by the\\nepoch’s end. This pattern was consistently applied across all\\n17 epochs. The training loss exhibited a steady decline, start-\\ning from 7.152 and progressively tapering down to 2.620,\\nreflecting the model’s increasing proficiency in learning\\nfrom the data. The purpose of this stage is to be able to\\nmake the Phi-2 model in TinyGPT-V react in some way to\\nthe input of the imaging modality. The alignment of text\\nand image in the semantic space is done.\\nStage 2: Comprising 4 epochs, each with 5000 iterations,\\nthis stage introduced the “linear_warmup_cosine_lr“ (He\\net al., 2018; Goyal et al., 2018) learning rate schedule. We\\ninitiated a warmup phase of 5000 steps, where the learn-\\ning rate linearly increased from 1e-6 (warmup_lr) to 1e-4\\n(init_lr), followed by a cosine decay down to a minimum\\nlearning rate of 8e-5. This phase saw a consistent reduction\\nin loss, starting at 2.726 and culminating at 2.343. The\\npurpose of this stage is to enable the LoRA module to play\\na role in multimodal data, further reducing the model’s loss\\non image-text pairs and improving the model’s ability to\\nlearn from the data.\\nStage 3: This stage lasted for 5 epochs, each with 200\\niterations. We maintained the “linear_warmup_cosine_lr“\\nschedule, with a warmup phase of 200 steps. The learning\\nrate began at 1e-6, ascending to 3e-5 (init_lr), before decay-\\ning to 1e-5 (min_lr). The loss values reflected significant\\n6'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 6}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nimprovements, starting at 1.992 and reducing to 1.125. The\\npurpose of this stage is to allow TinyGPT-V to accept both\\nverbal and image modal inputs and produce responses to\\nthem. After this stage of training TinyGPT-V has been able\\nto perform most of the image answering tasks.\\nStage 4: The final stage stretched over 50 epochs, each\\ncomprising 1000 iterations. We adhered to the “lin-\\near_warmup_cosine_lr“ schedule with a 1000-step warmup\\nphase. The learning rate was initiated at 1e-6, reaching up\\nto 1e-5 (init_lr), and then experiencing a cosine decay to\\na minimum of 8e-5. The training loss values displayed a\\nconsistent downward trajectory, beginning at 2.720 and ulti-\\nmately reaching as low as 1.399. The purpose of this stage is\\nto allow TinyGPT-V to perform various tasks such as VQA\\nor VSR tasks at the same time, increasing the generalization\\nperformance of TinyGPT-V on multimodal tasks.\\n4.2. Evaluation\\nEvaluation datasets. GQA (Hudson & Manning, 2019)\\nis a dataset for real-world visual reasoning and composi-\\ntional question answering, featuring a powerful question en-\\ngine that generates 22 million diverse reasoning questions.\\nVSR (Liu et al., 2023b) comprises over 10k natural text-\\nimage pairs in English, encompassing 66 types of spatial\\nrelations. IconQA (Lu et al., 2021) with 107,439 questions\\naimed at challenging visual understanding and reasoning in\\nthe context of icon images, encompassing three sub-tasks\\n(multi-image-choice, multi-text-choice, and filling-in-the-\\nblank). VizWiz (Gurari et al., 2018) is a collection of more\\nthan 31,000 visual queries, each derived from a photo taken\\nby a visually impaired individual using a smartphone, ac-\\ncompanied by a vocalized question regarding the image,\\nand supplemented with 10 answers sourced from a crowd\\nfor each query. The Hateful Memes dataset (HM) (Kiela\\net al., 2021), developed by Facebook AI, is a comprehensive\\nmultimodal collection specifically designed for the detec-\\ntion of hateful content in memes, combining both image\\nand text elements, and comprises over 10,000 newly created\\nmultimodal examples.\\nVisual question answering results. As shown in Table 2,\\nit becomes evident that TinyGPT-V , a language model with\\nonly 2.8 billion parameters, exhibits notably competitive per-\\nformance across multiple benchmarks, closely rivaling mod-\\nels with nearly 13 billion parameters. Specifically, in the\\nVSR (Visual-Spatial Reasoning) zero-shot task, TinyGPT-V\\noutshines its counterparts by securing the highest score of\\n54.7%. This is particularly impressive considering its pa-\\nrameter size is approximately 4.6 times smaller than other\\nleading models such as BLIP-2, LLaV A, and InstructBLIP.\\nIn the GQA benchmark, while TinyGPT-V scores are 38.9%,\\nit lags behind the highest score achieved by InstructBLIP,\\nwhich is 49.5%. However, TinyGPT-V shows robust perfor-\\nTinyGPT-V and others answer example compare\\nUres [vqa] where should I hide in this room when playing hide and\\nseek\\nLLaV A-1.5hide behind the bookshelf\\nMiniGPT-v2behind couch\\nGPT-4V Behind the Couch\\nUnder the Table\\nInside the Bookshelf\\nBehind the Curtains\\nBehind the TV\\nTinyGPT-Vunder couch\\nFigure 8: Comparison of reasoning answers from different\\nModels. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\nmance in the IconVQ challenge, attaining a score of 44.7%,\\njust 0.1% short of InstructBLIP’s leading score of 44.8%.\\nSimilarly, in the VizWiz task, TinyGPT-V demonstrates\\ncommendable capabilities with a score of 37.8%, which,\\nis not only the highest but is notable given its reduced pa-\\nrameter count. In the context of the Hateful Memes (HM)\\ndataset, TinyGPT-V matches InstructBLIP’s top score of\\n57.5% with its own score of 54.0%, again underscoring its\\nefficiency and capacity to compete with models of larger\\nscales. Overall, TinyGPT-V’s performance across these di-\\nverse and challenging benchmarks is striking, especially\\nwhen considering its parameter efficiency\\n4.3. Qualitative Evaluation\\nThe comparative analysis revealed TinyGPT-V’s distinct\\nadvantage in delivering concise and accurate visual interpre-\\ntations. In the reasoning task to find a hiding spot during\\na game of hide and seek, TinyGPT-V demonstrated its su-\\nperior capability by providing a singular, viable suggestion:\\n’under couch’. This contrasts with other models that either\\noffered multiple options, some of which were incorrect as\\nindicated by the text in red (e.g., GPT-4V suggesting ’In-\\nside the Bookshelf’), or specified less practical hiding spots.\\nWhen asked about potential activities in an image with an\\nalligator, TinyGPT-V suggested a cautious response with-\\nout speculating beyond what was visible. In contrast, other\\nmodels, like LLaV A-1.5, provided extended narratives that\\nintroduced assumptions not directly inferred from the image.\\nSimilarly, in describing a soccer match scene, TinyGPT-V’s\\nresponse was succinct and focused on the key elements,\\navoiding the inaccuracies noted in MiniGPT-v2’s account,\\nwhich incorrectly identified multiple soccer balls on the\\n7'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 7}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nMethod LLM GQA VSR IconVQ VizWiz HM AverageParameters (zero-shot) (zero-shot) (zero-shot) (zero-shot)\\nFlamingo 9B - 31.8 - 28.8 57.0 39.20\\nIDEFICS (Laurençon et al., 2023) 7B - 38.4 - 35.5 - 37.05\\n65B - 45.2 - 36.0 - 39.60\\nBLIP-2 13B 41.0 50.9 40.6 19.6 53.7 41.16\\nLLaV A 13B 41.3 51.2 43.0 - - 45.17\\nInstructBLIP 13B 49.5 52.1 44.8 33.4 57.5 47.45\\nMiniGPT-4 13B - - 35.9 - - 35.90\\nBLIV A (Hu et al., 2023) 7B - - 44.8 31.4 55.6 41.15\\nLLaV A-Phi (Zhu et al., 2024) 2.8B - - 54.1 37.6 - 43.15\\nMoE-LLaV A (Lin et al., 2024)∗ 1.8B×4 61.5 - - 32.6 - 47.50\\nOurs\\nTinyGPT-V (Phi-2) 2.8B 38.9 54.7 44.7 37.8 54.0 46.02\\nTinyGPT-V (Phi-1.5) 1.3B 34.3 35.8 37.2 28.4 50.3 37.2\\nTable 2: Comparative performance of TinyGPT-V and other MLLMs across multiple visual question answering benchmarks.\\n∗It is worth noting that MoE-LLaV A is required 8xA100-80G for training.\\nMethod TinyGPT-V LLaV A MiniGPT-4\\nseconds per words 0.067 0.426 0.300\\ninference occupancy (8-bit) 5.6GB 22GB 23.5GB\\nTable 3: Comparison of inference time and inference occu-\\npancy about devices.\\npitch. These examples, as tabulated in Table 5 and Table 7,\\nillustrate TinyGPT-V’s superior performance in generating\\nbrief yet precise responses, underscoring its practicality for\\nrapid and reliable visual question answering. For efficient\\nevaluation, as shown in table 3, TinyGPT-V operates at\\nthe fastest pace, taking only 0.067 seconds to generate a\\nword, which suggests upper efficiency in processing speed\\ncompared to LLaV A and MiniGPT-4. On the other hand,\\nLLaV A exhibits a significantly slower word generation time\\nat 0.426 seconds per word, coupled with a higher memory\\noccupancy of 22GB. MiniGPT-4, with a generation time of\\n0.300 seconds per word and a memory usage of 23.5GB.\\n4.4. Ablation Study\\nAs shown in Table 4, the full TinyGPT-V model achieves\\nlow loss across all stages, but the removal of key modules\\nleads to significant training issues. Without the LoRA mod-\\nule, there’s a gradient vanish starting from Stage 3. Omitting\\nInput Layer Norm increases loss notably (to 2.839 in Stage\\n1) and causes gradient vanishing in Stage 4. Without RMS\\nNorm, the model sees an elevated loss in Stage 1 (2.747)\\nand faces early gradient vanishing in Stage 2. The absence\\nof QK Norm results in immediate gradient vanish. This data\\nclearly illustrates each module’s crucial role in preventing\\ngradient vanishing and maintaining low loss throughout the\\ntraining process.\\nMethod Stage 1 Loss Stage 2 Loss Stage 3 Loss Stage 4 LossTinyGPT-V 2.620 2.343 1.125 1.330w/o LoRA 2.620 - Gradient Vanish -w/o Input Layer Norm 2.839 2.555 1.344 Gradient Vanishw/o RMS Norm 2.747 Gradient Vanish - -w/o QK Norm Gradient Vanish - - -\\nTable 4: Importance of each module in TinyGPT-V at each\\nstage of training.\\nFurthermore, our reveal a notable trend: the smaller the\\nlarge language model used for transfer learning (particu-\\nlarly in transitioning from text-to-image modality), the more\\nchallenging the training process becomes. We observed a\\npronounced need for additional normalization layers to sta-\\nbilize the training, especially when scaling down from larger\\nmodels like Vicuna-13B to smaller ones like Phi-2 (2.7B),\\nPhi-1.5 (1.3B), and other small backbones as detailed in the\\nAppendix B.\\n5. Conclusion\\nIn this study, we introduce TinyGPT-V , a parameter-efficient\\nMLLMs tailored for a range of real-world vision-language\\napplications. Our model innovatively builds on the compact\\nyet powerful Phi-2 small language model framework. This\\napproach results in TinyGPT-V delivering exceptional out-\\ncomes in diverse benchmarks like visual question-answering\\nand referring expression comprehension while keeping com-\\nputational demands manageable. Remarkably, TinyGPT-V\\ncan be trained on a 24G GPU and deployed on an 8G device,\\ndemonstrating a significant advancement in creating cost-\\neffective, efficient, and potent MLLMs. This paper marks a\\ncontribution towards crafting smaller, yet robust multimodal\\nlanguage models for practical, real-world use cases. We\\nenvision that our work will catalyze further explorations\\ninto developing compact MLLMs for diverse applications.\\n8'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 8}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nReferences\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\\nHasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., et al. Flamingo: a visual language model for few-shot\\nlearning. Advances in Neural Information Processing\\nSystems, 35:23716–23736, 2022.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:\\n1877–1901, 2020.\\nCao, Y ., Kang, Y ., Wang, C., and Sun, L. Instruction mining:\\nWhen data mining meets large language model finetuning,\\n2023.\\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\\nceptual 12m: Pushing web-scale image-text pre-training\\nto recognize long-tail visual concepts, 2021.\\nChen, J., Guo, H., Yi, K., Li, B., and Elhoseiny, M. Visual-\\ngpt: Data-efficient adaptation of pretrained language mod-\\nels for image captioning. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npp. 18030–18040, 2022.\\nChen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krish-\\nnamoorthi, R., Chandra, V ., Xiong, Y ., and Elhoseiny, M.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478, 2023.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y ., Gonzalez, J. E.,\\net al. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\\n(accessed 14 April 2023), 2023.\\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\\nW., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\\ngeneral-purpose vision-language models with instruction\\ntuning, 2023.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\nDufter, P., Schmitt, M., and Schütze, H. Position informa-\\ntion in transformers: An overview, 2021.\\nFang, Y ., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,\\nHuang, T., Wang, X., and Cao, Y . Eva: Exploring the\\nlimits of masked visual representation learning at scale,\\n2022.\\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P.,\\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and\\nHe, K. Accurate, large minibatch sgd: Training imagenet\\nin 1 hour, 2018.\\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\\nAnswering visual questions from blind people. In Pro-\\nceedings of the IEEE conference on computer vision and\\npattern recognition, pp. 3608–3617, 2018.\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classification with convolutional\\nneural networks, 2018.\\nHenry, A., Dachapally, P. R., Pawar, S., and Chen, Y . Query-\\nkey normalization for transformers, 2020.\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556,\\n2022.\\nHu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,\\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation\\nof large language models, 2021.\\nHu, W., Xu, Y ., Li, Y ., Li, W., Chen, Z., and Tu, Z. Bliva:\\nA simple multimodal llm for better handling of text-rich\\nvisual questions, 2023.\\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\\nreal-world visual reasoning and compositional question\\nanswering. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pp. 6700–\\n6709, 2019.\\nJavaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck,\\nS., Mendes, C. C. T., Chen, W., Giorno, A. D., Eldan,\\nR., Gopi, S., Gunasekar, S., Javaheripi, M., Kauff-\\nmann, P., Lee, Y . T., Li, Y ., Nguyen, A., de Rosa, G.,\\nSaarikivi, O., Salim, A., Shah, S., Santacroce, M.,\\nBehl, H. S., Kalai, A. T., Wang, X., Ward, R., Witte,\\nP., Zhang, C., and Zhang, Y . Phi-2: The surprising\\npower of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/\\nphi-2-the-surprising-power-of-small-language-models/ ,\\n2023.\\nKazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T.\\nReferitgame: Referring to objects in photographs of natu-\\nral scenes. In Proceedings of the 2014 conference on em-\\npirical methods in natural language processing (EMNLP),\\npp. 787–798, 2014.\\n9'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 9}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\nAdvances in neural information processing systems, 33:\\n2611–2624, 2020.\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes,\\n2021.\\nLaurençon, H., Saulnier, L., Tronchon, L., Bekman, S.,\\nSingh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush,\\nA. M., Kiela, D., Cord, M., and Sanh, V . Obelics: An\\nopen web-scale filtered dataset of interleaved image-text\\ndocuments, 2023.\\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\\nlanguage-image pre-training with frozen image encoders\\nand large language models, 2023a.\\nLi, Y ., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar,\\nS., and Lee, Y . T. Textbooks are all you need ii: phi-1.5\\ntechnical report, 2023b.\\nLin, B., Tang, Z., Ye, Y ., Cui, J., Zhu, B., Jin, P., Huang, J.,\\nZhang, J., Ning, M., and Yuan, L. Moe-llava: Mixture of\\nexperts for large vision-language models, 2024.\\nLin, T.-Y ., Maire, M., Belongie, S., Bourdev, L., Girshick,\\nR., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and\\nDollár, P. Microsoft coco: Common objects in context,\\n2015.\\nLiu, H., Li, C., Li, Y ., and Lee, Y . J. Improved baselines\\nwith visual instruction tuning, 2023a.\\nLiu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction\\ntuning. arXiv preprint arXiv:2304.08485, 2023b.\\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\\nfor abstract diagram understanding and visual language\\nreasoning. arXiv preprint arXiv:2110.13214, 2021.\\nMao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L.,\\nand Murphy, K. Generation and comprehension of unam-\\nbiguous object descriptions. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 11–20, 2016.\\nOpenAI. Introducing chatgpt. https://openai.com/\\nblog/chatgpt, 2022.\\nOrdonez, V ., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs. In\\nShawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and\\nWeinberger, K. (eds.), Advances in Neural Information\\nProcessing Systems, volume 24. Curran Associates, Inc.,\\n2011. URL https://proceedings.neurips.\\ncc/paper_files/paper/2011/file/\\n5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.\\npdf.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information\\nProcessing Systems, 35:27730–27744, 2022.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\\nSutskever, I., et al. Language models are unsupervised\\nmultitask learners. OpenAI blog, 1(8):9, 2019.\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\\nJ., Krueger, G., and Sutskever, I. Learning transferable\\nvisual models from natural language supervision, 2021.\\nSchuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,\\nR., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and\\nKomatsuzaki, A. Laion-400m: Open dataset of clip-\\nfiltered 400 million image-text pairs, 2021.\\nSchwenk, D., Khandelwal, A., Clark, C., Marino, K., and\\nMottaghi, R. A-okvqa: A benchmark for visual ques-\\ntion answering using world knowledge. In European\\nConference on Computer Vision, pp. 146–162. Springer,\\n2022.\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Gurevych,\\nI. and Miyao, Y . (eds.), Proceedings of the 56th An-\\nnual Meeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers), pp. 2556–2565, Mel-\\nbourne, Australia, July 2018. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/P18-1238. URL\\nhttps://aclanthology.org/P18-1238.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X.,\\nGuestrin, C., Liang, P., and Hashimoto, T. B. Stanford\\nalpaca: An instruction-following llama model, 2023.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV ., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y .,\\nMao, Y ., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\n10'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 10}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023.\\nTsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S.,\\nVinyals, O., and Hill, F. Multimodal few-shot learn-\\ning with frozen language models. Advances in Neural\\nInformation Processing Systems, 34:200–212, 2021.\\nWei, L., Jiang, Z., Huang, W., and Sun, L. Instructiongpt-\\n4: A 200-instruction paradigm for fine-tuning minigpt-4,\\n2023.\\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E.,\\nIli´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon,\\nF., et al. Bloom: A 176b-parameter open-access multilin-\\ngual language model. arXiv preprint arXiv:2211.05100,\\n2022.\\nYang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and\\nWang, L. The dawn of lmms: Preliminary explorations\\nwith gpt-4v(ision), 2023.\\nYu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L.\\nModeling context in referring expressions. In Computer\\nVision–ECCV 2016: 14th European Conference, Amster-\\ndam, The Netherlands, October 11-14, 2016, Proceed-\\nings, Part II 14, pp. 69–85. Springer, 2016.\\nYuan, Z., Wang, X., Wang, K., Sun, L., and Ye, Y . Artgpt-\\n4: Towards artistic-understanding large vision-language\\nmodels with enhanced adapter, 2023.\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,\\net al. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068, 2022.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z.,\\nZhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,\\nH., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\\nwith mt-bench and chatbot arena, 2023.\\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\\nMinigpt-4: Enhancing vision-language understanding\\nwith advanced large language models. arXiv preprint\\narXiv:2304.10592, 2023.\\nZhu, Y ., Zhu, M., Liu, N., Ou, Z., Mou, X., and Tang,\\nJ. Llava-phi: Efficient multi-modal assistant with small\\nlanguage model, 2024.\\n11'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 11}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nA. Multi-task Instruction Template\\nTo reduce ambiguity in training a unified multimodal model for diverse tasks such as visual question answering, image\\ncaptioning, referring expression comprehension, generation, and object parsing and grounding, we employed task-specific\\ntokens from MiniGPT-v2 within a multitask instruction template. This template, derived from the LLaMA-2 conversation\\ntemplate (Touvron et al., 2023), includes a general input format comprising image features, a task identifier token, and an\\ninstruction input. We incorporated six distinct task identifiers, each associated with a particular task. For tasks necessitating\\nthe identification of spatial locations of referred objects, the model uses textual representations of bounding boxes with\\ncoordinates normalized between 0 and 100. Overall, MiniGPT-v2’s unique task-specific tokens enhance task disambiguation,\\nleading to more precise and effective task execution.\\nTinyGPT-V and others answer example compare\\nUres What might happen in this image in the next second\\nLLaV A-1.5 In the image, a man is sitting on a log next to a large alligator, and he is holding a camera. It is likely\\nthat the man is about to take a photograph or record a video of the alligator. The next second could\\ninvolve the man capturing the alligator’s behavior, appearance, or any other interesting moment.\\nThe alligator might also react to the man’s presence, such as moving or showing curiosity, which\\ncould make the captured image or video more engaging and memorable.\\nMiniGPT-v2 In the next second, the man might be startled or surprised by the close-up shot of the crocodile\\nin the water. This could cause him to jump back or react with caution, ensuring his safety in the\\npresence of the wild animal. The image captures a moment of wonder and curiosity, highlighting\\nthe beauty of the natural environment while also acknowledging the potential danger that comes\\nwith being so close to a wild animal.\\nGPT-4V In the next second, the man might quickly move away to maintain a safe distance from the alligator.\\nAlligators can be unpredictable, and it’s important to keep a safe distance from them at all times to\\navoid potential harm. The man seems to be taking a selfie with the alligator, which suggests that he\\nis close to the animal. This is generally not advisable as it can be dangerous. It’s always important\\nto respect wildlife and observe it from a distance that is safe for both the person and the animal.\\nTinyGPT-V The image shows a man holding a selfie camera next to a crocodile. This could be an unusual and\\nunexpected sight, as crocodiles are not typically associated with human activities like taking selfies.\\nlt’s important for the man to stay safe and avoid any potential risks posed by the crocodile.\\nTable 5: Comparison of prediction answers from different models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\nB. Small Backbones for Transfer Learning\\nAs shown in Table 6, a striking pattern emerges from the data: smaller LLMs exhibit heightened sensitivity to the removal\\nof these modules, with a pronounced tendency towards training difficulties, such as gradient vanishing. For instance, the\\nabsence of LoRA in both Phi-1.5 and TinyLLaMA resulted in an immediate cessation of training progress post-Stage 1,\\nindicating a critical reliance on this module for sustaining training in smaller models. Similarly, the exclusion of QK Norm\\nled to gradient vanishing at the earliest stage across all smaller LLMs, underscoring its essential role in the initial phases\\n12'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 12}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nof training. Moreover, the sequential progression in training losses across stages for models without these modifications\\ndemonstrates a clear degradation in training efficiency and effectiveness. For example, the removal of Input Layer Norm and\\nRMS Norm not only heightened Stage 1 loss across Phi-1.5 and TinyLLaMA but also precipitated gradient vanishing in\\nlater stages, showcasing the compound impact of these modules on model stability and learning capability. This analysis\\nincontrovertibly highlights a fundamental challenge in training smaller LLMs for migration to MLLMs: the absence of\\nkey architectural and normalization modules severely impedes their training process, making them more prone to early\\ntraining halts and efficiency losses. The results underscore the necessity of these components in supporting the stability and\\ngradual learning progression of smaller LLMs, thus illuminating a pivotal consideration for developers aiming to optimize\\nthe training framework for seamless model.\\nLLM Stage 1 Loss Stage 2 Loss Stage 3 Loss Stage 4 Loss\\nPhi-2 (2.7B) 2.620 2.343 1.125 1.330\\nPhi-1.5 (1.3B) 3.420 3.043 1.525 1.730\\nw/o LoRA 3.420 - Gradient Vanish -\\nw/o Input Layer Norm 3.555 3.221 1.544 Gradient Vanish\\nw/o RMS Norm 3.557 Gradient Vanish - -\\nw/o QK Norm Gradient Vanish - - -\\nTinyLLaMA (1.1B) 3.529 3.053 1.371 1.830\\nw/o LoRA 3.529 - Gradient Vanish -\\nw/o Input Layer Norm 3.611 3.331 1.444 Gradient Vanish\\nw/o RMS Norm 3.607 Gradient Vanish - -\\nw/o QK Norm Gradient Vanish - - -\\nTable 6: Importance of each module and different LLMs in TinyGPT-V training framework at each stage of training.\\n13'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 13}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nTinyGPT-V and others answer example compare\\nUres What’s in the image?\\nLLaV A-1.5 The image shows a group of men playing soccer on a field. They are actively engaged in the game,\\nwith some players running towards the soccer ball.\\nMiniGPT-v2 The image features a lively and competitive soccer game with several players on the field. Some of\\nthe players are wearing jerseys, while others have their own unique outfits. There are at least eight\\nplayers in total, with three of them being the main focus of the scene.\\nOne player is kicking a soccer ball, with another player nearby trying to block the shot. A third\\nplayer is also present, playing offense. In addition, there are two soccer balls visible on the field,\\none closer to the left side and the other near the center of the scene.\\nThe players are engaged in intense action, showcasing their skills and determination to score. The\\natmosphere appears to be a high-energy match with a mix of excitement and competition among\\nthe players.\\nGPT-4V The image shows a moment from a soccer match. In the foreground, there’s a player in a blue\\nuniform who appears to be dribbling the ball with close control, evading two opponents in white\\nuniforms. The player in blue is crouched low to the ground, demonstrating agility and focus, which\\nsuggests he is in the midst of making a strategic play, possibly attempting to bypass the defenders. In\\nthe background, other players can be observed, and the stands are filled with spectators, indicating\\nthat this is a professional match likely taking place in a large stadium. The attire and hairstyles\\nsuggest the photo may be from a past era, possibly the 1980s or 1990s.\\nTinyGPT-V The image shows a socer game with several people playing, including at least one person in a blue\\nshirt. The players are engaged in a friendly match, showcasing their skills and teamwork.\\nTable 7: Comparison of descriptive answers from different Models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\n14')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract image as text"
      ],
      "metadata": {
        "id": "DUTT_av8qvj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidocr-onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noTcp2G9qjjX",
        "outputId": "3e420880-65c4-4688-991b-6b7eddbda4f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidocr-onnxruntime\n",
            "  Downloading rapidocr_onnxruntime-1.4.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr-onnxruntime)\n",
            "  Downloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (4.10.0.84)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (1.16.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (2.0.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (6.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (11.0.0)\n",
            "Collecting onnxruntime>=1.7.0 (from rapidocr-onnxruntime)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (4.66.6)\n",
            "Collecting coloredlogs (from onnxruntime>=1.7.0->rapidocr-onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
            "Downloading rapidocr_onnxruntime-1.4.1-py3-none-any.whl (14.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyclipper, humanfriendly, coloredlogs, onnxruntime, rapidocr-onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1 pyclipper-1.3.0.post6 rapidocr-onnxruntime-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load unstructed pdf"
      ],
      "metadata": {
        "id": "ozYfCLp4th9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elINfJxsttIl",
        "outputId": "a065e2b0-9767-49bd-a26e-714ab807aa4a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PDFMinerLoader\n",
        "\n",
        "loader = PDFMinerLoader(url)\n",
        "data = loader.load()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBj14xOWtkm-",
        "outputId": "1952dd07-4362-4f2d-ccaf-bc24bd371e8e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf'}, page_content='TinyGPT-V: Efficient Multimodal Large Language Model\\nvia Small Backbones\\n\\nZhengqing Yuan 1 Zhaoxu Li 2 Weiran Huang 3 Yanfang Ye 1 Lichao Sun 4\\n\\n4\\n2\\n0\\n2\\n\\nn\\nu\\nJ\\n\\n1\\n2\\n\\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n\\n3\\nv\\n2\\n6\\n8\\n6\\n1\\n.\\n2\\n1\\n3\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nIn recent years, multimodal large language mod-\\nels (MLLMs) such as GPT-4V have demonstrated\\nremarkable advancements, excelling in a variety\\nof vision-language tasks. Despite their prowess,\\nthe closed-source nature and computational de-\\nmands of such models limit their accessibility and\\napplicability. This study introduces TinyGPT-V,\\na novel open-source MLLM, designed for effi-\\ncient training and inference across various vision-\\nlanguage tasks, including image captioning (IC)\\nand visual question answering (VQA). Leverag-\\ning a compact yet powerful architecture, TinyGPT-\\nV integrates the Phi-2 language model with pre-\\ntrained vision encoders, utilizing a unique map-\\nping module for visual and linguistic information\\nfusion. With a training regimen optimized for\\nsmall backbones and employing a diverse dataset\\namalgam, TinyGPT-V requires significantly lower\\ncomputational resources—24GB for training and\\nas little as 8GB for inference—without compro-\\nmising on performance. Our experiments demon-\\nstrate that TinyGPT-V, with its language model\\n2.8 billion parameters, achieves comparable re-\\nsults in VQA and image inference tasks to its\\nlarger counterparts while being uniquely suited\\nfor deployment on resource-constrained devices\\nthrough innovative quantization techniques. This\\nwork not only paves the way for more accessible\\nand efficient MLLMs but also underscores the po-\\ntential of smaller, optimized models in bridging\\nthe gap between high performance and computa-\\ntional efficiency in real-world applications. Addi-\\ntionally, this paper introduces a new approach to\\nmultimodal large language models using smaller\\nbackbones. Our code and training weights are\\navailable in the supplementary material.\\n\\n1Universiy of Notre Dame 2Nanyang Technological University\\n3Shanghai Jiao Tong University 4Lehigh University. Correspon-\\ndence to: Lichao Sun <lis221@lehigh.edu>.\\n\\nAccepted to the Workshop on Advancing Neural Network Training\\nat International Conference on Machine Learning (WANT@ICML\\n2024).\\n\\n1\\n\\nFigure 1: Comparison of TinyGPT-V with other current\\nMLLMs models shows TinyGPT-V achieves cost-effective,\\nefficient, and high-performing with fewer parameters.\\n\\n1. Introduction\\n\\nIn recent years, the field of artificial intelligence has seen\\nsignificant advancements through the development of multi-\\nmodal large language models (MLLMs), such as GPT-4V,\\nwhich have shown exceptional performance across a range\\nof vision-language tasks (Yang et al., 2023). Despite GPT-\\n4V’s impressive capabilities, its closed-source nature limits\\nits widespread application and adaptability. In contrast, the\\nopen-source landscape for MLLMs is rapidly evolving, pre-\\nsenting models like LLaVA and MiniGPT-4 that excel in\\nimage captioning (IC), visual question answering (VQA)\\noften comparable GPT-4V in these areas (Dai et al., 2023;\\nLiu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-\\nv2 (Chen et al., 2023) has demonstrated superior perfor-\\nmance in various visual grounding and question-answering\\ntasks. However, its training code remains proprietary, which\\nposes challenges for community-driven advancements and\\nadaptability.\\n\\nAlthough the impressive vision-language capabilities\\ndemonstrated by some open-source MLLMs, they fre-\\nquently necessitate significant computational resources for\\ntraining and inference. For example, training LLaVA-v1.5-\\n13B (Liu et al., 2023a) required 8 × A100 GPUs, each\\nequipped with 80GB of memory, cumulating in 25.5 hours\\nof continuous training. As shown in Figure 2 (a), the un-\\n\\n5.07.510.012.515.0Parameter (Billions)3840424446PerformanceFlamingoBLIP-2LLaVAInstructBLIPMiniGPT-4TinyGPT-V \\n \\n \\n \\n \\n \\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nFigure 2: (a) is the occupancy ratio of each component in MiniGPT-4, and (b) is the occupancy ratio of each component in\\nTinyGPT-V. We have considerably narrowed down the occupancy ratio of the Language model in MLLMs.\\n\\nderlying performance of large language models, which are\\nintegral to MLLMs, is pivotal. Models such as LLaVA-\\nv1.5-13B and MiniGPT-v2 depend on high-capacity back-\\nbones which are Vicuna-13b-v1.5 (Zheng et al., 2023) and\\nLLaMA2-7B-Chat (Touvron et al., 2023), respectively, ne-\\ncessitating a substantial number of parameters to effectively\\ntackle complex tasks including IC and VQA.\\n\\nWe introduce TinyGPT-V, a novel model designed for effi-\\ncient training and inference, requiring only 24GB of GPU\\nmemory for training and as little as 8GB of GPU or CPU\\nmemory for inference. This model makes use of the ad-\\nvanced large language model Phi-2 (Javaheripi et al., 2023)\\nand incorporates pre-trained vision modules from (Li et al.,\\n2023a) and CLIP (Radford et al., 2021) as its vision encoder,\\ncoupled with a mapping module to facilitate the integration\\nof visual information. During training, TinyGPT-V adopts\\na novel training methodology focused on small pre-trained\\nbackbones, unlike any other MLLMs, utilizing the unique\\nmapping module between the visual encoder and the lan-\\nguage model as well as novelty normalization methods,\\nwhile keeping all other components frozen. For its train-\\ning dataset, TinyGPT-V employs the multi-tasks datasets,\\nincluding LAION (Schuhmann et al., 2021), Conceptual\\nCaptions (Changpinyo et al., 2021; Sharma et al., 2018),\\nSBU (Ordonez et al., 2011), and others (Lin et al., 2015;\\nSchwenk et al., 2022; Hudson & Manning, 2019; Kiela et al.,\\n2020; Lu et al., 2021; Gurari et al., 2018; Mao et al., 2016;\\nKazemzadeh et al., 2014; Yu et al., 2016).\\n\\nIn our study, we found that TinyGPT-V exhibits similar traits\\nwith GPT-4, especially when doing some VQA and image\\ninference. With only 2.8 billion parameters of its language\\nmodel, TinyGPT-V employs a unique quantization process,\\nlike using 8-bit quantization, making it well-suited for local\\ndeployment and inference on 8GB mobile devices. This\\nmodel represents a significant advancement in achieving a\\n\\nbalance between exceptional performance and efficiency in\\nMLLMs, as shown in Figure 1. Our work not only aims\\nto enable the community to develop more cost-effective,\\nefficient, and high-performing MLLMs for widespread real-\\nworld applications but also introduces a training framework\\noptimized for small pre-trained backbones.\\n\\n2. Related Work\\n\\nAdvanced language model. The evolution of language\\nmodels has been marked by significant milestones, start-\\ning with early successes like GPT2 (Radford et al., 2019)\\nand BERT (Devlin et al., 2018) in natural language process-\\ning (NLP). These foundational models set the stage for the\\nsubsequent development of vastly larger language models,\\nencompassing hundreds of billions of parameters. This dra-\\nmatic increase in scale has led to the emergence of advanced\\ncapabilities as seen in models like GPT-3 (Brown et al.,\\n2020), Chinchilla (Hoffmann et al., 2022), OPT (Zhang\\net al., 2022), and BLOOM (Workshop et al., 2022). These\\nlarge language models (LLMs) have been instrumental\\nin further advancements in the field. For instance, Chat-\\nGPT (OpenAI, 2022) and InstructGPT (Ouyang et al., 2022)\\nleverage these powerful models to answer diverse ques-\\ntions and perform complex tasks such as coding. The in-\\ntroduction of open-source LLMs like LLaMA (Touvron\\net al., 2023) has further propelled research in this area, in-\\nspiring subsequent developments like Alpaca (Taori et al.,\\n2023), Vicuna (Chiang et al., 2023). These models fine-tune\\nthe LLaMA model with additional high-quality instruction\\ndatasets, showcasing the versatility and adaptability of LLM\\nframeworks.Among the most notable recent advancements\\nare Phi (Li et al., 2023b) and its successor, Phi-2 (Javaheripi\\net al., 2023). These models have demonstrated exceptional\\nperformance, rivaling or even surpassing models up to 25\\ntimes larger in scale. This indicates a significant shift in\\n\\n2\\n\\nProjection (110M)0.8%EVA-VIT (0.98B)7.0%Vicuna(13B)92.3%Projection (119M)3.1%EVA-VIT (0.98B)25.1%Phi-2(2.8B)71.8%(a) MiniGPT-4(a) TinyGPT-V\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nFigure 3: Compared to other general-purpose MLLMs, our TinyGPT-V achieves the same performance as 13B or 7B models\\nin a variety of visual language tasks.\\n\\nthe landscape of language modeling, emphasizing efficiency\\nand effectiveness without necessarily relying on sheer size.\\n\\nMultimodal language model. In recent years, the trend of\\naligning visual input to large language models for vision-\\nlanguage tasks has gained significant attention (Chen et al.,\\n2022; Tsimpoukelli et al., 2021; Alayrac et al., 2022; Li\\net al., 2023a; Liu et al., 2023b;a; Zhu et al., 2023; Chen\\net al., 2023). Seminal works like VisualGPT (Chen et al.,\\n2022) and Frozen (Tsimpoukelli et al., 2021), which uti-\\nlized pre-trained language models for image captioning and\\nvisual question answering. This approach was further ad-\\nvanced by models such as Flamingo (Alayrac et al., 2022),\\nwhich incorporated gated cross-attention mechanisms to\\nalign pre-trained vision encoders and language models, train-\\ning on vast image-text pairs. BLIP-2 (Li et al., 2023a)\\nintroduced an efficient Q-Former for aligning visual and\\nlanguage modalities. These groundbreaking studies have\\npaved the way for further innovations in the field, leading to\\nthe development of models like LLaVA (Liu et al., 2023b)\\nand MiniGPT4 (Zhu et al., 2023), and their subsequent iter-\\nations, LLaVA-v1.5 (Liu et al., 2023a), MiniGPT-v2 (Chen\\net al., 2023), ArtGPT-4 (Yuan et al., 2023), instruction GPT-\\n4 (Wei et al., 2023) and Instruction Mining (Cao et al.,\\n2023). These models have demonstrated advanced multi-\\nmodal capabilities through instruction tuning, showcasing\\nremarkable generalization abilities.\\n\\n3. Method\\n\\nWe briefly introduce our vision-language model, TinyGPT-\\nV, followed by an analysis of its structure, culminating in a\\ndetailed description of the training process for each stage.\\n\\n3.1. Model Architecture\\n\\nIn this subsection, we present the architecture of TinyGPT-V,\\nwhich consists of a visual encoder, projection layers, and a\\nlarge language model, as shown in Figure 4.\\n\\nVisual encoder backbone. In the TinyGPT-V, it utilizes\\nEVA (Fang et al., 2022) of the ViT serves as the visual\\nfoundation model, which remains inactive during the entire\\ntraining process. Our model operates at an image resolution\\nof 224x224 for Stages 1, 2, and 3, and at 448x448 for Stage\\n4. The positional encoding is enhanced to accommodate the\\nincreased image resolution which is known as the Relative\\nPosition Bias (Dufter et al., 2021). It enhances the model’s\\nunderstanding of the spatial relationships between elements\\nin an image.\\n\\nProjection layers. The Projection layers embed visual\\nfeatures extracted by the visual encoder into the language\\nmodel, enhancing the model’s ability to process image-\\nbased information. We adopt the Q-Former layers from\\nthe BLIP-2 architecture (Li et al., 2023a) as the initial pro-\\n\\n3\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nOur studies indicate that these smaller models are prone to\\nencountering NaN or INF values during multimodal data\\ncomputations. This issue often leads to a computational\\nloss value of NaN, thereby causing failure in the initial\\nbatch forward propagation. Moreover, the limited number\\nof trainable parameters in these models may lead to gradient\\nvanishing during training. To mitigate these problems, as\\ndepicted in Figure 5 (c), we incorporate the post-norm and\\ninput norm mechanisms from LLaMA-2, applying RMS\\nNorm after each Multi-Head Attention Layer (MHA) to\\nnormalize data for downstream processing. In addition, we\\nhave to update all layer norms in the Phi-2 model to improve\\ntraining stability, as detailed in the subsequent equation.\\n\\nLayerNorminput(xhidden) = γ\\n\\nxhidden − µ\\nσ2 + ϵ\\n\\n√\\n\\n+ β\\n\\n(1)\\n\\nWhere, xhidden is the input of this layer, µ and σ2 are the\\nmean and variance of the inputs to the layer, respectively, ϵ\\nis a small number to prevent division by zero, γ and β are\\ntrainable parameters.\\n\\nRMSNorm(xpost) =\\n\\n(cid:113)\\n\\n1\\nN\\n\\nxpost\\n(cid:80)N\\n\\ni=1 x2\\n\\ni + ϵ\\n\\n(2)\\n\\nwhere xpost is the input after MHA, N is the dimension of\\nxpost.\\n\\nFurthermore, (Henry et al., 2020) have underscored the vital\\nrole of Query-Key Normalization in low-resource learning\\nscenarios. Hence, as show in Figure 5 (d), we have incor-\\nporated Query-Key Normalization into the Phi-2 model, as\\ndetailed in the following equation.\\n\\nAttention(Q, K, V ) = softmax\\n\\n(cid:18) LayerNorm(Q)LayerNorm(K)T\\n√\\n\\n(cid:19)\\n\\nV\\n\\ndk\\n\\n(3)\\n\\nwhere dk denotes the dimension of Q or K.\\n\\nThe structure of the LoRA mechanism (Hu et al., 2021)\\nis show in Figure 5 (a), which is an efficient fine-tuning\\nmethod in parallel to the frozen pre-training weights as\\nshown in Figure 5 (c), which does not increase the inference\\ntime consuming for large language models and is easier to\\noptimize.\\n\\n3.2. Training Stages\\n\\nIn this subsection,\\nTinyGPT-V will be described.\\n\\nthe four-stage training process of\\n\\nWarm-up training for the first training stage. During\\nthe initial pretraining stage, TinyGPT-V is taught vision-\\nlanguage understanding using large datasets of aligned\\n\\n4\\n\\nFigure 4: Architecture of TinyGPT-V. The model takes a\\nvisual backbone, which remains frozen during all training\\nphases. We concatenate Q-Former layer visual output tokens\\nfrom ViT backbone and project them into Phi-2 language\\nmodel space via two linear projection layers.\\n\\njection layer, aiming to leverage the full potential of the\\npre-trained BLIP system within visual language models.\\nThis strategy significantly reduces the number of parameters\\nneeding training. The second and third layers are linear\\nprojection layers, designed to bridge the dimensionality gap\\nbetween the Q-Former output and the language model’s\\nembedding layer, thereby aligning visual tokens more effec-\\ntively with the language model’s hidden space. As shown in\\nFigure 6, to expedite TinyGPT-V’s training, we initially use\\na pre-trained Linear Projection from MiniGPT-4 (Vicuna\\n7B) as the second layer. We then introduce an additional lin-\\near projection layer, initialized with a Gaussian distribution,\\nas the third layer to seamlessly integrate into the hidden\\nspace of the Phi-2 model.\\n\\nLarge lanuguage model backbone. Our TinyGPT-V large\\nlanguage model is built upon the Phi-2 model (Javaheripi\\net al., 2023) as its backbone. Phi-2, a 2.7 billion-parameter\\nlanguage model, exhibits exceptional reasoning and lan-\\nguage comprehension abilities, achieving state-of-the-art\\nperformance among language models with fewer than 13\\nbillion parameters. In complex benchmarks, Phi-2 either\\nmatches or exceeds the performance of models up to 25\\ntimes its size. We primarily use Phi-2’s linguistic abilities\\nto do various vision-language tasks. Specifically, for vision\\nreasoning tasks that involve spatial location identification,\\nwe instruct the linguistic model to generate textual descrip-\\ntions of what will happen in the next scenario, representing\\ntheir objects’ coordinates, as shown in Table 8.\\n\\nNormalization and LoRA for TinyGPT-V. In Section 4.4,\\nwe conclude that training smaller-scale large language mod-\\nels for transfer learning, especially across different modali-\\nties (e.g., from text to image), poses significant challenges.\\n\\nPhi-2LoRA+NormalizationFreezeTrainQ-FormerViTLinear 1 (MiniGPT-4)Linear 2This is an image of an alpaca. Alpacas are domesticated species of South American camelids, known for their soft fluffy coats which are used for making wool.[vqa]What’s this?\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nFigure 5: (a) represents the structure of LoRA, (b) represents how LoRA can efficiently fine-tune large language models\\n(LLMs) in natural language processing, (c) represents the structure of LLMs for TinyGPT-V, and (d) represents the structure\\nof QK Normalization.\\n\\ninitial training stage, the large language model becomes\\nequipped to process image modality inputs. To guarantee\\nmore consistent performance as the model transitions into\\nthe subsequent training stage, we re-employ the dataset from\\nthe first stage, specifically for training the LoRA module.\\n\\nInstruction tuning for the third training stage. We fine-\\ntuned this TinyGPT-V model using a selection of image-text\\npairings from MiniGPT4 or LLaVA, which included in-\\nstructions like “###Human: <Img><ImageHere></Img>\\nTake a look at this image and describe what you no-\\ntice.###Assistant:.”. We used a uniform template inclusive\\nof a randomly chosen prompt that improved the model’s\\ncapacity for generating responses that were consistent and\\nsounded more natural.\\n\\nMulti-task learning in the fourth training stage. The\\nfourth training stage of TinyGPT-V focuses on enhancing\\nits conversation ability as a chatbot by tuning the model with\\nmore multi-modal instruction datasets as shown in Table 1,\\nincluding LLaVA, Flickr30k, a mixing multi-task dataset,\\nand Unnatural Instruction using multi-tasks template as de-\\ntailed in appendix A. The LLaVA dataset is utilized for\\nmulti-modal instruction tuning with detailed descriptions\\nand complex reasoning examples. The Flickr30k dataset\\nis used to improve grounded image caption generation and\\nobject parsing and grounding capabilities. Additionally, a\\nmixing multi-task dataset is created to improve the model’s\\nhandling of multiple tasks during multi-round conversations.\\nFinally, to recover the language generation ability, the Un-\\nnatural Instruction dataset is added to the third-stage training\\nof TinyGPT-V.\\n\\nFigure 6: The training process of TinyGPT-V, the first stage\\nis warm-up training, the second stage is pre-training, the\\nthird stage is instruction fine-tuning, and the fourth stage is\\nmulti-task learning.\\n\\nimage-text pairs. The model identifies the output from the\\nprojection layers as a soft prompt directing it to create rel-\\nevant texts and to allow large language models to accept\\ninputs from the image modality. The pretraining process\\nuses a dataset combination of Conceptual Caption, SBU,\\nand LAION, involving 20000 training steps covering about\\n5 million image-text pairs.\\n\\nPre-training for the second training stage. Following the\\n\\n5\\n\\nLinearDownrLinearUpPretrainedWeightsInputOutputLoRA(a) LoRAMLPNormalizationMHALoRA(b) LoRA Module for LLMs BlockMHAQuery-Key NormalizationLayer NormRMS NormMLPLoRAAfter Stage1(c) LLMs Block for TinyGPT-VAttentionQKVLayer NormLayer Norm(d) Query-Key Normalization for MHAFreezeTrainDataPathwayConditional PathwayPhi-2Image-text Pair Instruction LearningFreezeTrainStage 1Visual Encode & Q-FormerNormalizationMiniGPT-4 Proj.LinearPhi-2Stage 2Visual Encode & Q-FormerNormalizationMiniGPT-4 Proj.Linear& LoRAPhi-2Visual Encode & Q-FormerNormalizationMiniGPT-4 Proj.Linear& LoRAStage 3Phi-2Visual Encode & Q-FormerNormalizationMiniGPT-4 Proj.Linear& LoRAMulti-Tasks LearningStage 4\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nDataset\\nData types\\nLAION, CC3M, SBU\\nImage-text pair\\nMiniGPT-4 Stage2 for CC & SBU\\nInstruction tuning\\nText Captions, COCO Captions\\nCaption\\nRefCOCO, RefCOCO+, RefCOCOg, Visual Genome\\nREC\\nGQA, VQAv2, OK-VQA, AOK-VQA, OCR-VQA\\nVQA\\nMultimodal instruction LLaVA dataset, Flickr30k, Multi-task conversation\\nUnnatural Instructions\\nLangauge dataset\\n\\nStage 1\\n✓\\n✗\\n✗\\n✗\\n✗\\n✗\\n✗\\n\\nStage 2\\n✓\\n✗\\n✗\\n✗\\n✗\\n✗\\n✗\\n\\nStage 3\\n✗\\n✓\\n✗\\n✗\\n✗\\n✗\\n✗\\n\\nStage 4\\n✗\\n✗\\n✓\\n✓\\n✓\\n✓\\n✓\\n\\nTable 1: The full list of datasets used by TinyGPT-MoE during training.\\n\\nFigure 7: Changes in loss during the training stage of TinyGPT-V.\\n\\n4. Experiments\\n\\nIn this section, we describe the training and evaluation meth-\\nods in detail.\\n\\n4.1. Training\\n\\nExperimental setting. The experimental environment for\\nthis study was established with a single NVIDIA RTX 3090\\nGPU, equipped with a substantial 24GB of VRAM. The cen-\\ntral processing was handled by an AMD EPYC 7552 48-core\\nProcessor, offering 15 virtual CPUs. Memory allocation was\\nset at 80GB, ensuring sufficient capacity for handling large\\ndatasets. The software environment was standardized on\\nPyTorch version 2.0.0, with CUDA 11.8 support, facilitating\\noptimized tensor operations on the GPU.\\n\\nTraining process. In our experimental process, we meticu-\\nlously orchestrated the training of our model through four\\ndistinct stages, each characterized by specific learning rate\\nstrategies and loss profiles, as shown in Figure 7.\\n\\nStage 1: Spanning 17 epochs, with each epoch consisting\\nof 1000 iterations, we employed a dynamic learning rate\\napproach. The learning rate commenced at 1e-5 at the begin-\\nning of each epoch and gradually ascended to 1e-4 by the\\nepoch’s end. This pattern was consistently applied across all\\n\\n17 epochs. The training loss exhibited a steady decline, start-\\ning from 7.152 and progressively tapering down to 2.620,\\nreflecting the model’s increasing proficiency in learning\\nfrom the data. The purpose of this stage is to be able to\\nmake the Phi-2 model in TinyGPT-V react in some way to\\nthe input of the imaging modality. The alignment of text\\nand image in the semantic space is done.\\n\\nStage 2: Comprising 4 epochs, each with 5000 iterations,\\nthis stage introduced the “linear_warmup_cosine_lr“ (He\\net al., 2018; Goyal et al., 2018) learning rate schedule. We\\ninitiated a warmup phase of 5000 steps, where the learn-\\ning rate linearly increased from 1e-6 (warmup_lr) to 1e-4\\n(init_lr), followed by a cosine decay down to a minimum\\nlearning rate of 8e-5. This phase saw a consistent reduction\\nin loss, starting at 2.726 and culminating at 2.343. The\\npurpose of this stage is to enable the LoRA module to play\\na role in multimodal data, further reducing the model’s loss\\non image-text pairs and improving the model’s ability to\\nlearn from the data.\\n\\nStage 3: This stage lasted for 5 epochs, each with 200\\niterations. We maintained the “linear_warmup_cosine_lr“\\nschedule, with a warmup phase of 200 steps. The learning\\nrate began at 1e-6, ascending to 3e-5 (init_lr), before decay-\\ning to 1e-5 (min_lr). The loss values reflected significant\\n\\n6\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nimprovements, starting at 1.992 and reducing to 1.125. The\\npurpose of this stage is to allow TinyGPT-V to accept both\\nverbal and image modal inputs and produce responses to\\nthem. After this stage of training TinyGPT-V has been able\\nto perform most of the image answering tasks.\\n\\nStage 4: The final stage stretched over 50 epochs, each\\ncomprising 1000 iterations. We adhered to the “lin-\\near_warmup_cosine_lr“ schedule with a 1000-step warmup\\nphase. The learning rate was initiated at 1e-6, reaching up\\nto 1e-5 (init_lr), and then experiencing a cosine decay to\\na minimum of 8e-5. The training loss values displayed a\\nconsistent downward trajectory, beginning at 2.720 and ulti-\\nmately reaching as low as 1.399. The purpose of this stage is\\nto allow TinyGPT-V to perform various tasks such as VQA\\nor VSR tasks at the same time, increasing the generalization\\nperformance of TinyGPT-V on multimodal tasks.\\n\\n4.2. Evaluation\\n\\nEvaluation datasets. GQA (Hudson & Manning, 2019)\\nis a dataset for real-world visual reasoning and composi-\\ntional question answering, featuring a powerful question en-\\ngine that generates 22 million diverse reasoning questions.\\nVSR (Liu et al., 2023b) comprises over 10k natural text-\\nimage pairs in English, encompassing 66 types of spatial\\nrelations. IconQA (Lu et al., 2021) with 107,439 questions\\naimed at challenging visual understanding and reasoning in\\nthe context of icon images, encompassing three sub-tasks\\n(multi-image-choice, multi-text-choice, and filling-in-the-\\nblank). VizWiz (Gurari et al., 2018) is a collection of more\\nthan 31,000 visual queries, each derived from a photo taken\\nby a visually impaired individual using a smartphone, ac-\\ncompanied by a vocalized question regarding the image,\\nand supplemented with 10 answers sourced from a crowd\\nfor each query. The Hateful Memes dataset (HM) (Kiela\\net al., 2021), developed by Facebook AI, is a comprehensive\\nmultimodal collection specifically designed for the detec-\\ntion of hateful content in memes, combining both image\\nand text elements, and comprises over 10,000 newly created\\nmultimodal examples.\\n\\nVisual question answering results. As shown in Table 2,\\nit becomes evident that TinyGPT-V, a language model with\\nonly 2.8 billion parameters, exhibits notably competitive per-\\nformance across multiple benchmarks, closely rivaling mod-\\nels with nearly 13 billion parameters. Specifically, in the\\nVSR (Visual-Spatial Reasoning) zero-shot task, TinyGPT-V\\noutshines its counterparts by securing the highest score of\\n54.7%. This is particularly impressive considering its pa-\\nrameter size is approximately 4.6 times smaller than other\\nleading models such as BLIP-2, LLaVA, and InstructBLIP.\\nIn the GQA benchmark, while TinyGPT-V scores are 38.9%,\\nit lags behind the highest score achieved by InstructBLIP,\\nwhich is 49.5%. However, TinyGPT-V shows robust perfor-\\n\\nTinyGPT-V and others answer example compare\\nUres\\n\\n[vqa] where should I hide in this room when playing hide and\\nseek\\n\\nLLaVA-1.5\\nMiniGPT-v2\\nGPT-4V\\n\\nTinyGPT-V\\n\\nhide behind the bookshelf\\nbehind couch\\nBehind the Couch\\nUnder the Table\\nInside the Bookshelf\\nBehind the Curtains\\nBehind the TV\\nunder couch\\n\\nFigure 8: Comparison of reasoning answers from different\\nModels. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\n\\nmance in the IconVQ challenge, attaining a score of 44.7%,\\njust 0.1% short of InstructBLIP’s leading score of 44.8%.\\nSimilarly, in the VizWiz task, TinyGPT-V demonstrates\\ncommendable capabilities with a score of 37.8%, which,\\nis not only the highest but is notable given its reduced pa-\\nrameter count. In the context of the Hateful Memes (HM)\\ndataset, TinyGPT-V matches InstructBLIP’s top score of\\n57.5% with its own score of 54.0%, again underscoring its\\nefficiency and capacity to compete with models of larger\\nscales. Overall, TinyGPT-V’s performance across these di-\\nverse and challenging benchmarks is striking, especially\\nwhen considering its parameter efficiency\\n\\n4.3. Qualitative Evaluation\\n\\nThe comparative analysis revealed TinyGPT-V’s distinct\\nadvantage in delivering concise and accurate visual interpre-\\ntations. In the reasoning task to find a hiding spot during\\na game of hide and seek, TinyGPT-V demonstrated its su-\\nperior capability by providing a singular, viable suggestion:\\n’under couch’. This contrasts with other models that either\\noffered multiple options, some of which were incorrect as\\nindicated by the text in red (e.g., GPT-4V suggesting ’In-\\nside the Bookshelf’), or specified less practical hiding spots.\\nWhen asked about potential activities in an image with an\\nalligator, TinyGPT-V suggested a cautious response with-\\nout speculating beyond what was visible. In contrast, other\\nmodels, like LLaVA-1.5, provided extended narratives that\\nintroduced assumptions not directly inferred from the image.\\nSimilarly, in describing a soccer match scene, TinyGPT-V’s\\nresponse was succinct and focused on the key elements,\\navoiding the inaccuracies noted in MiniGPT-v2’s account,\\nwhich incorrectly identified multiple soccer balls on the\\n\\n7\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nMethod\\n\\nFlamingo\\nIDEFICS (Laurençon et al., 2023)\\n\\nBLIP-2\\nLLaVA\\nInstructBLIP\\nMiniGPT-4\\nBLIVA (Hu et al., 2023)\\nLLaVA-Phi (Zhu et al., 2024)\\nMoE-LLaVA (Lin et al., 2024)∗\\nOurs\\nTinyGPT-V (Phi-2)\\nTinyGPT-V (Phi-1.5)\\n\\nLLM\\nParameters\\n9B\\n7B\\n65B\\n13B\\n13B\\n13B\\n13B\\n7B\\n2.8B\\n1.8B×4\\n\\n2.8B\\n1.3B\\n\\nGQA\\n\\n-\\n-\\n-\\n41.0\\n41.3\\n49.5\\n-\\n-\\n-\\n61.5\\n\\n38.9\\n34.3\\n\\nVSR\\n(zero-shot)\\n31.8\\n38.4\\n45.2\\n50.9\\n51.2\\n52.1\\n-\\n-\\n-\\n-\\n\\nIconVQ\\n(zero-shot)\\n-\\n-\\n-\\n40.6\\n43.0\\n44.8\\n35.9\\n44.8\\n54.1\\n-\\n\\nVizWiz\\n(zero-shot)\\n28.8\\n35.5\\n36.0\\n19.6\\n-\\n33.4\\n-\\n31.4\\n37.6\\n32.6\\n\\nHM\\n(zero-shot)\\n57.0\\n-\\n-\\n53.7\\n-\\n57.5\\n-\\n55.6\\n-\\n-\\n\\n54.7\\n35.8\\n\\n44.7\\n37.2\\n\\n37.8\\n28.4\\n\\n54.0\\n50.3\\n\\nAverage\\n\\n39.20\\n37.05\\n39.60\\n41.16\\n45.17\\n47.45\\n35.90\\n41.15\\n43.15\\n47.50\\n\\n46.02\\n37.2\\n\\nTable 2: Comparative performance of TinyGPT-V and other MLLMs across multiple visual question answering benchmarks.\\n∗It is worth noting that MoE-LLaVA is required 8xA100-80G for training.\\n\\nMethod\\n\\nTinyGPT-V LLaVA MiniGPT-4\\n\\nseconds per words\\n\\n0.067\\n\\ninference occupancy (8-bit)\\n\\n5.6GB\\n\\n0.426\\n\\n22GB\\n\\n0.300\\n\\n23.5GB\\n\\nTable 3: Comparison of inference time and inference occu-\\npancy about devices.\\n\\nMethod\\nTinyGPT-V\\nw/o LoRA\\nw/o Input Layer Norm\\nw/o RMS Norm\\nw/o QK Norm\\n\\nStage 1 Loss\\n2.620\\n2.620\\n2.839\\n2.747\\nGradient Vanish\\n\\nStage 2 Loss\\n2.343\\n-\\n2.555\\nGradient Vanish\\n-\\n\\nStage 3 Loss\\n1.125\\nGradient Vanish\\n1.344\\n-\\n-\\n\\nStage 4 Loss\\n1.330\\n-\\nGradient Vanish\\n-\\n-\\n\\nTable 4: Importance of each module in TinyGPT-V at each\\nstage of training.\\n\\npitch. These examples, as tabulated in Table 5 and Table 7,\\nillustrate TinyGPT-V’s superior performance in generating\\nbrief yet precise responses, underscoring its practicality for\\nrapid and reliable visual question answering. For efficient\\nevaluation, as shown in table 3, TinyGPT-V operates at\\nthe fastest pace, taking only 0.067 seconds to generate a\\nword, which suggests upper efficiency in processing speed\\ncompared to LLaVA and MiniGPT-4. On the other hand,\\nLLaVA exhibits a significantly slower word generation time\\nat 0.426 seconds per word, coupled with a higher memory\\noccupancy of 22GB. MiniGPT-4, with a generation time of\\n0.300 seconds per word and a memory usage of 23.5GB.\\n\\n4.4. Ablation Study\\n\\nAs shown in Table 4, the full TinyGPT-V model achieves\\nlow loss across all stages, but the removal of key modules\\nleads to significant training issues. Without the LoRA mod-\\nule, there’s a gradient vanish starting from Stage 3. Omitting\\nInput Layer Norm increases loss notably (to 2.839 in Stage\\n1) and causes gradient vanishing in Stage 4. Without RMS\\nNorm, the model sees an elevated loss in Stage 1 (2.747)\\nand faces early gradient vanishing in Stage 2. The absence\\nof QK Norm results in immediate gradient vanish. This data\\nclearly illustrates each module’s crucial role in preventing\\ngradient vanishing and maintaining low loss throughout the\\ntraining process.\\n\\nFurthermore, our reveal a notable trend: the smaller the\\nlarge language model used for transfer learning (particu-\\nlarly in transitioning from text-to-image modality), the more\\nchallenging the training process becomes. We observed a\\npronounced need for additional normalization layers to sta-\\nbilize the training, especially when scaling down from larger\\nmodels like Vicuna-13B to smaller ones like Phi-2 (2.7B),\\nPhi-1.5 (1.3B), and other small backbones as detailed in the\\nAppendix B.\\n\\n5. Conclusion\\n\\nIn this study, we introduce TinyGPT-V, a parameter-efficient\\nMLLMs tailored for a range of real-world vision-language\\napplications. Our model innovatively builds on the compact\\nyet powerful Phi-2 small language model framework. This\\napproach results in TinyGPT-V delivering exceptional out-\\ncomes in diverse benchmarks like visual question-answering\\nand referring expression comprehension while keeping com-\\nputational demands manageable. Remarkably, TinyGPT-V\\ncan be trained on a 24G GPU and deployed on an 8G device,\\ndemonstrating a significant advancement in creating cost-\\neffective, efficient, and potent MLLMs. This paper marks a\\ncontribution towards crafting smaller, yet robust multimodal\\nlanguage models for practical, real-world use cases. We\\nenvision that our work will catalyze further explorations\\ninto developing compact MLLMs for diverse applications.\\n\\n8\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nReferences\\n\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., et al. Flamingo: a visual language model for few-shot\\nlearning. Advances in Neural Information Processing\\nSystems, 35:23716–23736, 2022.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:\\n1877–1901, 2020.\\n\\nCao, Y., Kang, Y., Wang, C., and Sun, L. Instruction mining:\\nWhen data mining meets large language model finetuning,\\n2023.\\n\\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\\nceptual 12m: Pushing web-scale image-text pre-training\\nto recognize long-tail visual concepts, 2021.\\n\\nChen, J., Guo, H., Yi, K., Li, B., and Elhoseiny, M. Visual-\\ngpt: Data-efficient adaptation of pretrained language mod-\\nels for image captioning. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npp. 18030–18040, 2022.\\n\\nChen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krish-\\nnamoorthi, R., Chandra, V., Xiong, Y., and Elhoseiny, M.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478, 2023.\\n\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\\net al. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\\n(accessed 14 April 2023), 2023.\\n\\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\\nW., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\\ngeneral-purpose vision-language models with instruction\\ntuning, 2023.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\n\\nDufter, P., Schmitt, M., and Schütze, H. Position informa-\\n\\ntion in transformers: An overview, 2021.\\n\\nFang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,\\nHuang, T., Wang, X., and Cao, Y. Eva: Exploring the\\nlimits of masked visual representation learning at scale,\\n2022.\\n\\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P.,\\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and\\nHe, K. Accurate, large minibatch sgd: Training imagenet\\nin 1 hour, 2018.\\n\\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\\nAnswering visual questions from blind people. In Pro-\\nceedings of the IEEE conference on computer vision and\\npattern recognition, pp. 3608–3617, 2018.\\n\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classification with convolutional\\nneural networks, 2018.\\n\\nHenry, A., Dachapally, P. R., Pawar, S., and Chen, Y. Query-\\n\\nkey normalization for transformers, 2020.\\n\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556,\\n2022.\\n\\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation\\nof large language models, 2021.\\n\\nHu, W., Xu, Y., Li, Y., Li, W., Chen, Z., and Tu, Z. Bliva:\\nA simple multimodal llm for better handling of text-rich\\nvisual questions, 2023.\\n\\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\\nreal-world visual reasoning and compositional question\\nanswering. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pp. 6700–\\n6709, 2019.\\n\\nJavaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck,\\nS., Mendes, C. C. T., Chen, W., Giorno, A. D., Eldan,\\nR., Gopi, S., Gunasekar, S., Javaheripi, M., Kauff-\\nmann, P., Lee, Y. T., Li, Y., Nguyen, A., de Rosa, G.,\\nSaarikivi, O., Salim, A., Shah, S., Santacroce, M.,\\nBehl, H. S., Kalai, A. T., Wang, X., Ward, R., Witte,\\nP., Zhang, C., and Zhang, Y. Phi-2: The surprising\\npower of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/\\nphi-2-the-surprising-power-of-small-language-models/,\\n2023.\\n\\nKazemzadeh, S., Ordonez, V., Matten, M., and Berg, T.\\nReferitgame: Referring to objects in photographs of natu-\\nral scenes. In Proceedings of the 2014 conference on em-\\npirical methods in natural language processing (EMNLP),\\npp. 787–798, 2014.\\n\\n9\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nKiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\nAdvances in neural information processing systems, 33:\\n2611–2624, 2020.\\n\\nProcessing Systems, volume 24. Curran Associates, Inc.,\\n2011. URL https://proceedings.neurips.\\ncc/paper_files/paper/2011/file/\\n5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.\\npdf.\\n\\nKiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes,\\n2021.\\n\\nLaurençon, H., Saulnier, L., Tronchon, L., Bekman, S.,\\nSingh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush,\\nA. M., Kiela, D., Cord, M., and Sanh, V. Obelics: An\\nopen web-scale filtered dataset of interleaved image-text\\ndocuments, 2023.\\n\\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\\nlanguage-image pre-training with frozen image encoders\\nand large language models, 2023a.\\n\\nLi, Y., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar,\\nS., and Lee, Y. T. Textbooks are all you need ii: phi-1.5\\ntechnical report, 2023b.\\n\\nLin, B., Tang, Z., Ye, Y., Cui, J., Zhu, B., Jin, P., Huang, J.,\\nZhang, J., Ning, M., and Yuan, L. Moe-llava: Mixture of\\nexperts for large vision-language models, 2024.\\n\\nLin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick,\\nR., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and\\nDollár, P. Microsoft coco: Common objects in context,\\n2015.\\n\\nLiu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines\\n\\nwith visual instruction tuning, 2023a.\\n\\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\\n\\ntuning. arXiv preprint arXiv:2304.08485, 2023b.\\n\\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu,\\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\\nfor abstract diagram understanding and visual language\\nreasoning. arXiv preprint arXiv:2110.13214, 2021.\\n\\nMao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L.,\\nand Murphy, K. Generation and comprehension of unam-\\nbiguous object descriptions. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 11–20, 2016.\\n\\nOpenAI. Introducing chatgpt. https://openai.com/\\n\\nblog/chatgpt, 2022.\\n\\nOrdonez, V., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs.\\nIn\\nShawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and\\nWeinberger, K. (eds.), Advances in Neural Information\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information\\nProcessing Systems, 35:27730–27744, 2022.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\\nSutskever, I., et al. Language models are unsupervised\\nmultitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\\nJ., Krueger, G., and Sutskever, I. Learning transferable\\nvisual models from natural language supervision, 2021.\\n\\nSchuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,\\nR., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and\\nKomatsuzaki, A. Laion-400m: Open dataset of clip-\\nfiltered 400 million image-text pairs, 2021.\\n\\nSchwenk, D., Khandelwal, A., Clark, C., Marino, K., and\\nMottaghi, R. A-okvqa: A benchmark for visual ques-\\nIn European\\ntion answering using world knowledge.\\nConference on Computer Vision, pp. 146–162. Springer,\\n2022.\\n\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Gurevych,\\nI. and Miyao, Y. (eds.), Proceedings of the 56th An-\\nnual Meeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers), pp. 2556–2565, Mel-\\nbourne, Australia, July 2018. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/P18-1238. URL\\nhttps://aclanthology.org/P18-1238.\\n\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,\\nGuestrin, C., Liang, P., and Hashimoto, T. B. Stanford\\nalpaca: An instruction-following llama model, 2023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\n\\n10\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023.\\n\\nTsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S.,\\nVinyals, O., and Hill, F. Multimodal few-shot learn-\\ning with frozen language models. Advances in Neural\\nInformation Processing Systems, 34:200–212, 2021.\\n\\nWei, L., Jiang, Z., Huang, W., and Sun, L. Instructiongpt-\\n4: A 200-instruction paradigm for fine-tuning minigpt-4,\\n2023.\\n\\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E.,\\nIli´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon,\\nF., et al. Bloom: A 176b-parameter open-access multilin-\\ngual language model. arXiv preprint arXiv:2211.05100,\\n2022.\\n\\nYang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and\\nWang, L. The dawn of lmms: Preliminary explorations\\nwith gpt-4v(ision), 2023.\\n\\nYu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L.\\nModeling context in referring expressions. In Computer\\nVision–ECCV 2016: 14th European Conference, Amster-\\ndam, The Netherlands, October 11-14, 2016, Proceed-\\nings, Part II 14, pp. 69–85. Springer, 2016.\\n\\nYuan, Z., Wang, X., Wang, K., Sun, L., and Ye, Y. Artgpt-\\n4: Towards artistic-understanding large vision-language\\nmodels with enhanced adapter, 2023.\\n\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\\net al. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068, 2022.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,\\nH., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\\nwith mt-bench and chatbot arena, 2023.\\n\\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\\nMinigpt-4: Enhancing vision-language understanding\\nwith advanced large language models. arXiv preprint\\narXiv:2304.10592, 2023.\\n\\nZhu, Y., Zhu, M., Liu, N., Ou, Z., Mou, X., and Tang,\\nJ. Llava-phi: Efficient multi-modal assistant with small\\nlanguage model, 2024.\\n\\n11\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nA. Multi-task Instruction Template\\n\\nTo reduce ambiguity in training a unified multimodal model for diverse tasks such as visual question answering, image\\ncaptioning, referring expression comprehension, generation, and object parsing and grounding, we employed task-specific\\ntokens from MiniGPT-v2 within a multitask instruction template. This template, derived from the LLaMA-2 conversation\\ntemplate (Touvron et al., 2023), includes a general input format comprising image features, a task identifier token, and an\\ninstruction input. We incorporated six distinct task identifiers, each associated with a particular task. For tasks necessitating\\nthe identification of spatial locations of referred objects, the model uses textual representations of bounding boxes with\\ncoordinates normalized between 0 and 100. Overall, MiniGPT-v2’s unique task-specific tokens enhance task disambiguation,\\nleading to more precise and effective task execution.\\n\\nTinyGPT-V and others answer example compare\\nUres\\n\\nWhat might happen in this image in the next second\\n\\nLLaVA-1.5\\n\\nMiniGPT-v2\\n\\nGPT-4V\\n\\nTinyGPT-V\\n\\nIn the image, a man is sitting on a log next to a large alligator, and he is holding a camera. It is likely\\nthat the man is about to take a photograph or record a video of the alligator. The next second could\\ninvolve the man capturing the alligator’s behavior, appearance, or any other interesting moment.\\nThe alligator might also react to the man’s presence, such as moving or showing curiosity, which\\ncould make the captured image or video more engaging and memorable.\\nIn the next second, the man might be startled or surprised by the close-up shot of the crocodile\\nin the water. This could cause him to jump back or react with caution, ensuring his safety in the\\npresence of the wild animal. The image captures a moment of wonder and curiosity, highlighting\\nthe beauty of the natural environment while also acknowledging the potential danger that comes\\nwith being so close to a wild animal.\\nIn the next second, the man might quickly move away to maintain a safe distance from the alligator.\\nAlligators can be unpredictable, and it’s important to keep a safe distance from them at all times to\\navoid potential harm. The man seems to be taking a selfie with the alligator, which suggests that he\\nis close to the animal. This is generally not advisable as it can be dangerous. It’s always important\\nto respect wildlife and observe it from a distance that is safe for both the person and the animal.\\nThe image shows a man holding a selfie camera next to a crocodile. This could be an unusual and\\nunexpected sight, as crocodiles are not typically associated with human activities like taking selfies.\\nlt’s important for the man to stay safe and avoid any potential risks posed by the crocodile.\\n\\nTable 5: Comparison of prediction answers from different models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\n\\nB. Small Backbones for Transfer Learning\\n\\nAs shown in Table 6, a striking pattern emerges from the data: smaller LLMs exhibit heightened sensitivity to the removal\\nof these modules, with a pronounced tendency towards training difficulties, such as gradient vanishing. For instance, the\\nabsence of LoRA in both Phi-1.5 and TinyLLaMA resulted in an immediate cessation of training progress post-Stage 1,\\nindicating a critical reliance on this module for sustaining training in smaller models. Similarly, the exclusion of QK Norm\\nled to gradient vanishing at the earliest stage across all smaller LLMs, underscoring its essential role in the initial phases\\n\\n12\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nof training. Moreover, the sequential progression in training losses across stages for models without these modifications\\ndemonstrates a clear degradation in training efficiency and effectiveness. For example, the removal of Input Layer Norm and\\nRMS Norm not only heightened Stage 1 loss across Phi-1.5 and TinyLLaMA but also precipitated gradient vanishing in\\nlater stages, showcasing the compound impact of these modules on model stability and learning capability. This analysis\\nincontrovertibly highlights a fundamental challenge in training smaller LLMs for migration to MLLMs: the absence of\\nkey architectural and normalization modules severely impedes their training process, making them more prone to early\\ntraining halts and efficiency losses. The results underscore the necessity of these components in supporting the stability and\\ngradual learning progression of smaller LLMs, thus illuminating a pivotal consideration for developers aiming to optimize\\nthe training framework for seamless model.\\n\\nLLM\\nPhi-2 (2.7B)\\nPhi-1.5 (1.3B)\\nw/o LoRA\\nw/o Input Layer Norm\\nw/o RMS Norm\\nw/o QK Norm\\nTinyLLaMA (1.1B)\\nw/o LoRA\\nw/o Input Layer Norm\\nw/o RMS Norm\\nw/o QK Norm\\n\\nStage 1 Loss\\n2.620\\n3.420\\n3.420\\n3.555\\n3.557\\nGradient Vanish\\n3.529\\n3.529\\n3.611\\n3.607\\nGradient Vanish\\n\\nStage 2 Loss\\n2.343\\n3.043\\n-\\n3.221\\nGradient Vanish\\n-\\n3.053\\n-\\n3.331\\nGradient Vanish\\n-\\n\\nStage 3 Loss\\n1.125\\n1.525\\nGradient Vanish\\n1.544\\n-\\n-\\n1.371\\nGradient Vanish\\n1.444\\n-\\n-\\n\\nStage 4 Loss\\n1.330\\n1.730\\n-\\nGradient Vanish\\n-\\n-\\n1.830\\n-\\nGradient Vanish\\n-\\n-\\n\\nTable 6: Importance of each module and different LLMs in TinyGPT-V training framework at each stage of training.\\n\\n13\\n\\n\\x0cSubmission and Formatting Instructions for the WANT@ICML 2024\\n\\nTinyGPT-V and others answer example compare\\nUres\\nWhat’s in the image?\\n\\nLLaVA-1.5\\n\\nThe image shows a group of men playing soccer on a field. They are actively engaged in the game,\\nwith some players running towards the soccer ball.\\n\\nMiniGPT-v2 The image features a lively and competitive soccer game with several players on the field. Some of\\nthe players are wearing jerseys, while others have their own unique outfits. There are at least eight\\nplayers in total, with three of them being the main focus of the scene.\\nOne player is kicking a soccer ball, with another player nearby trying to block the shot. A third\\nplayer is also present, playing offense. In addition, there are two soccer balls visible on the field,\\none closer to the left side and the other near the center of the scene.\\nThe players are engaged in intense action, showcasing their skills and determination to score. The\\natmosphere appears to be a high-energy match with a mix of excitement and competition among\\nthe players.\\nThe image shows a moment from a soccer match. In the foreground, there’s a player in a blue\\nuniform who appears to be dribbling the ball with close control, evading two opponents in white\\nuniforms. The player in blue is crouched low to the ground, demonstrating agility and focus, which\\nsuggests he is in the midst of making a strategic play, possibly attempting to bypass the defenders. In\\nthe background, other players can be observed, and the stands are filled with spectators, indicating\\nthat this is a professional match likely taking place in a large stadium. The attire and hairstyles\\nsuggest the photo may be from a past era, possibly the 1980s or 1990s.\\nThe image shows a socer game with several people playing, including at least one person in a blue\\nshirt. The players are engaged in a friendly match, showcasing their skills and teamwork.\\n\\nTinyGPT-V\\n\\nGPT-4V\\n\\nTable 7: Comparison of descriptive answers from different Models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\n\\n14\\n\\n\\x0c')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load file from directory"
      ],
      "metadata": {
        "id": "emsG-M81rCpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "loader = PyPDFDirectoryLoader(\"data/\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "515JpUqpq8v2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWcgDvSNrb_v",
        "outputId": "4699cea8-1d6b-472d-f21c-83a66a88d38b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'data/Alice Clark CV.pdf', 'page': 0}, page_content='Alice Clark \\nAI / Machine Learning \\n \\nDelhi, India Email me on Indeed \\n• 20+ years of experience in data handling, design, and development \\n• Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to \\ndata warehousing and business intelligence \\n• Database: Experience in database designing, scalability, back-up and recovery, writing and \\noptimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. \\nCloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, \\nStream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake \\nanalytics(U-SQL) \\nWilling to relocate anywhere \\n \\nWORK EXPERIENCE \\nSoftware Engineer \\nMicrosoft – Bangalore, Karnataka \\nJanuary 2000 to Present \\n1. Microsoft Rewards Live dashboards: \\nDescription: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping \\nonline. Microsoft Rewards members can earn points when searching with Bing, browsing with \\nMicrosoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft \\nStore. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft \\nrewards website. Rewards live dashboards gives a live picture of usage world-wide and by \\nmarkets like US, Canada, Australia, new user registration count, top/bottom performing rewards \\noffers, orders stats and weekly trends of user activities, orders and new user registrations. the \\nPBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. \\nTechnology/Tools used \\n \\nEDUCATION \\nIndian Institute of Technology – Mumbai \\n2001 \\n \\nSKILLS \\nMachine Learning, Natural Language Processing, and Big Data Handling \\n '),\n",
              " Document(metadata={'source': 'data/Alice Clark CV.pdf', 'page': 1}, page_content='ADDITIONAL INFORMATION \\nProfessional Skills \\n• Excellent analytical, problem solving, communication, knowledge transfer and interpersonal \\nskills with ability to interact with individuals at all the levels \\n• Quick learner and maintains cordial relationship with project manager and team members and \\ngood performer both in team and independent job environments \\n• Positive attitude towards superiors &amp; peers \\n• Supervised junior developers throughout project lifecycle and provided technical assistance '),\n",
              " Document(metadata={'source': 'data/Smith Resume.pdf', 'page': 0}, page_content='Michael Smith \\nBI / Big Data/ Azure \\nManchester, UK- Email me on Indeed: indeed.com/r/falicent/140749dace5dc26f \\n \\n10+ years of Experience in Designing, Development, Administration, Analysis, \\nManagement inthe Business Intelligence Da ta warehousing, Client Server \\nTechnologies, Web-based Applications, cloud solutions and Databases. \\nData warehouse: Data analysis, star/ snow flake schema data modeling and design \\nspecific todata warehousing and business intelligence environment. \\nDatabase: Experience in database designing, scalability, back -up and recovery, \\nwriting andoptimizing SQL code and Stored Procedures, creating functions, views, \\ntriggers and indexes.  \\nCloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL \\nAzure, StreamAnalytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure \\ndata lake analytics(U-SQL). \\nBig Data: Worked Azure data lake store/analytics for big data processing and Azure \\ndata factoryto schedule U-SQL jobs. Designed and developed end to end big data \\nsolution for data insights.  \\n \\nWilling to relocate: Anywhere \\nWORK EXPERIENCESoftware Engineer \\nMicrosoft - Manchester, UK. \\nDecember 2015 to Present \\n1. Microsoft Rewards Live dashboards: \\nDescription: - Microsoft rewards is loyalty program that rewards Users for \\nbrowsing and shopping online. Microsoft Rewards members can earn points when \\nsearching with Bing, browsing with Microsoft Edge and making purchases at the \\nXbox Store, the Windows St ore and the Microsoft Store. Plus, user can pick up \\nbonus points for taking daily quizzes and tours on the Microsoft rewards website. \\nRewards live dashboards gives a live picture of usage world -wide and by markets \\nlike US, Canada, Australia, new user regis tration count, top/bottom performing \\nrewards offers, orders stats and weekly trends of user activities, orders and new \\nuser registrations. the PBI tiles gets refreshed in different frequencies starting \\nfrom 5 seconds to 30 minutes. \\nTechnology/Tools used \\nEvent hub, stream analytics and Power BI. \\nResponsibilities \\nCreated stream analytics jobs to process event hub data \\nCreated Power BI live dashboard to show live usage traffic, weekly trends, cards, \\ncharts to showtop/bottom 10 offers and usage metrics. \\n2. Microsoft Rewards Data Insights: \\nDescription: - Microsoft rewards is loyalty program that rewards Users for \\nbrowsing and shopping online. Microsoft Rewards members can earn points when \\nsearching with Bing, browsing with Microsoft Edge and making purchases at t he \\nXbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up \\nbonus points for taking daily quizzes and tours on the Microsoft rewards website. \\nRewards data insights is data analytics and reporting platform, processes 20 \\nmillion users daily activities and redemption across different markets like US, \\nCanada, Australia. \\nTechnology/Tools used \\nCosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI. \\nResponsibilities '),\n",
              " Document(metadata={'source': 'data/Smith Resume.pdf', 'page': 1}, page_content='Created big data scripts in cosmos \\nC# data extractors, processors and reducers for data transformation \\nPower BI dashboards \\n3. End to end tracking Tool: \\nDescription: - This is real -time Tracking tool to track different business \\ntransactions like order, order response, functional acknowledgement, invoice \\nflowing inside ICOE. It gives flexibility to customers to track their transactions \\nand appropriate error information in-case of any failure. Based on resource based \\naccess control the tool gives flexibility to end user to perform different actions \\nlike view transactions, search based on different filter criteria and view and \\ndownload actual message payload. End to end tracking tool stitches all the \\nbusiness transaction like order to cash flow and connects different hops inside \\nICOE like gateway, routing server, Processing server. It also connects different \\nsystems like ICOE, partner end point and SAP. \\nTechnology/Tools used \\nAzure Document db, Azure web job and Web APP, RBAC, Angular JS. \\nResponsibilities \\nDocument dB stored procedures. \\nWeb job to process event hub data and populate Document db• Web App API. \\nStream analytics job to transform data \\nPower BI reports \\n4. Biztrack Tracking Tool: \\nDescription: - This is real -time Tracking tool to track different business \\ntransactions like order, order response, functional acknowledgement, invoice \\nflowing inside ICOE. It gives flexibility to customers to track their transactions \\nand appropriate error information in-case of any failure. Based on resource based \\naccess control the tool gives flexibility to end user to perform different actions \\nlike view transactions, search based on different filter criteria and view and \\ndownload actual message payload. \\nTechnology/Tools used \\nSQL server 2014, SSIS, .net API, Angular JS. \\nResponsibilities \\nETL solution to transform business transactions data stored in Biztalk tables. \\nSQL azure tables, stored procedures, User defined functions. \\nPerformance tuning. \\nWeb API enhancements. \\n \\nEDUCATION \\nThe University of Manchester - UK \\n2007 \\n \\nSKILLS \\nproblem solving (Less than 1 year), project lifecycle (Less than 1 year), project \\nmanager (Less than 1 year), technical assistance. (Less than 1 year) \\nADDITIONAL INFORMATION \\nProfessional Skills \\nExcellent analytical, problem solving, communication, knowledge transfer and \\ninterpersonalskills with ability to interact with individuals at all the levels \\nQuick learner and maintains cordial relationship with project manager and team \\nmembers andgood performer both in team and independent job environments \\nPositive attitude towards superiors &amp; peers '),\n",
              " Document(metadata={'source': 'data/Smith Resume.pdf', 'page': 2}, page_content='Supervised junior developers throughout project lifecycle and provided technical \\nassistance. ')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load web page"
      ],
      "metadata": {
        "id": "x3lhNRPSrtYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "web_paths = ['https://cs50.harvard.edu/ai/2020/']\n",
        "\n",
        "classes = []\n",
        "\n",
        "bs4_strainer = bs4.SoupStrainer(class_=classes)\n",
        "\n",
        "web_loader = WebBaseLoader(\n",
        "    web_paths,\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4_strainer\n",
        "    )\n",
        ")\n",
        "\n",
        "data = web_loader.load()"
      ],
      "metadata": {
        "id": "Y6af4OIErdH2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFzACmEtsp-Y",
        "outputId": "17161349-08f0-4572-8c4f-cefe04e0cae1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://cs50.harvard.edu/ai/2020/', 'title': \"CS50's Introduction to Artificial Intelligence with Python\", 'language': 'en-us'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCS50's Introduction to Artificial Intelligence with Python\\n\\n\\n\\nThis is an older version of the course. See cs50.harvard.edu/ai for the latest!\\n\\n\\n\\nCS50’s Introduction to Artificial Intelligence with Python\\nOpenCourseWare\\nDonate\\nBrian Yu\\nbrian@cs.harvard.edu\\nDavid J. Malan\\n\\nmalan@harvard.edu\\n\\nFacebook\\nGitHub\\nInstagram\\nLinkedIn\\nReddit\\nThreads\\nTwitter\\n\\n                        Menu\\n                    \\n\\n\\nReady Player 50\\nZoom Meetings\\n\\n\\n\\nSearch\\nKnowledge\\nUncertainty\\nOptimization\\nLearning\\nNeural Networks\\nLanguage\\n\\n\\n\\nAcademic Honesty\\nCS50 Certificate\\nFAQs\\nGradebook\\n\\n\\n\\nEd Discussion for Q&A\\nQuick Start Guide\\n\\n\\n\\nApple TV\\nedX\\nGoogle TV\\nHarvard Extension School\\nHarvard Summer School\\nYouTube\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStaff\\n\\n\\n\\n\\n\\nApple TV\\n\\nGoogle TV\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStatus Page\\n\\n\\n\\n\\nCommunities\\nBluesky\\nClubhouse\\nDiscord Q&A\\nEd Q&A\\nFacebook Group Q&A\\nFacebook Page\\nGitHub\\nGitter Q&A\\nInstagram\\nLinkedIn Group\\nLinkedIn Page\\nMedium\\nQuora\\nReddit Q&A\\nSlack Q&A\\nSnapchat\\nSoundCloud\\nStack Exchange Q&A\\nTelegram\\nTikTok\\nThreads\\nTwitter Account\\nTwitter Community\\nYouTube\\n\\n\\n\\n\\nCourses\\nCS50x\\nCS50 AI\\nCS50 Business\\nCS50 Cybersecurity\\nCS50 for Lawyers\\nCS50 Games\\nCS50 Python\\nCS50 R\\nCS50 Scratch\\nCS50 SQL\\nCS50 Technology\\nCS50 Web\\n\\n\\n\\n\\n\\n\\nLicense\\n2024-07-02 19:29:57\\n\\n\\n\\nWelcome\\nThis course explores the concepts and algorithms at the foundation of modern artificial intelligence, diving into the ideas that give rise to technologies like game-playing engines, handwriting recognition, and machine translation. Through hands-on projects, students gain exposure to the theory behind graph search algorithms, classification, optimization, reinforcement learning, and other topics in artificial intelligence and machine learning as they incorporate them into their own Python programs. By course’s end, students emerge with experience in libraries for machine learning as well as knowledge of artificial intelligence principles that enable them to design intelligent systems of their own.\\n\\nPrerequisites\\nCS50x or at least one year of experience with Python.\\n\\nWatch an introduction\\nHow to Take this Course\\nEven if you are not a student at Harvard, you are welcome to “take” this course for free via this OpenCourseWare by working your way through the course’s seven weeks of material. If you’d like to submit the course’s seven projects for feedback, be sure to create an edX account, if you haven’t already. Ask questions along the way via any of the course’s communities!\\n\\nIf interested in a verified certificate from edX, enroll at cs50.edx.org/ai instead.\\nIf interested in a professional certificate from edX, enroll at cs50.edx.org/programs/ai instead.\\nIf interested in transfer credit and accreditation from Harvard Extension School, register at web.dce.harvard.edu/extension/csci/e/80 instead.\\nIf interested in transfer credit and accreditation from Harvard Summer School, register at web.dce.harvard.edu/summer/csci/s/80 instead.\\n\\nHow to Teach this Course\\nIf you are a teacher, you are welcome to adopt or adapt these materials for your own course, per the license.\\n\\n\\n\\n\\n\")]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XiWuYCuCswft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}